diff --git a/.cargo/config.toml b/.cargo/config.toml
index 212d9d8..de9dd2c 100644
--- a/.cargo/config.toml
+++ b/.cargo/config.toml
@@ -1,32 +1,24 @@
 [target.thumbv7em-none-eabihf]
-# runner = "qemu-system-arm -machine netduinoplus2 -nographic -semihosting-config enable=on,target=native -kernel"
-
-# use this when building for qemu with a connection to a locally hosted server
-runner = "qemu-system-arm -serial tcp:localhost:4545 -machine netduinoplus2 -nographic -semihosting-config enable=on,target=native -kernel "
-
+runner = "qemu-system-arm -machine netduinoplus2 -nographic -semihosting-config enable=on,target=native -kernel"
 
 rustflags = [
   # This is needed if your flash or ram addresses are not aligned to 0x10000 in memory.x
   # See https://github.com/rust-embedded/cortex-m-quickstart/pull/95
-  "-C",
-  "link-arg=--nmagic",
+  "-C", "link-arg=--nmagic",
 
   # LLD (shipped with the Rust toolchain) is used as the default linker
-  "-C",
-  "link-arg=-Tlink.ld",
+  "-C", "link-arg=-Tlink.ld",
 
-  "-C",
-  "panic=unwind",
+  "-C", "panic=unwind",
 
-  "-C",
-  "codegen-units=1",
+  "-C", "codegen-units=1"
 ]
 
 [unstable]
 build-std = ["core", "alloc"]
 
 [build]
-target = "thumbv7em-none-eabihf" # Cortex-M4F and Cortex-M7F (with FPU)
+target = "thumbv7em-none-eabihf"     # Cortex-M4F and Cortex-M7F (with FPU)
 
 [profile.release]
 opt-level = 3
diff --git a/.vscode/settings.json b/.vscode/settings.json
index 21b9aa9..e1bb1f3 100644
--- a/.vscode/settings.json
+++ b/.vscode/settings.json
@@ -6,7 +6,6 @@
         "Kalman",
         "presults",
         "setpoint",
-        "Usart"
     ],
     "rust-analyzer.linkedProjects": [
         "./Cargo.toml"
diff --git a/Cargo.toml b/Cargo.toml
index eea933e..399494b 100644
--- a/Cargo.toml
+++ b/Cargo.toml
@@ -1,6 +1,6 @@
 [package]
 name = "hopter"
-version = "0.2.2"
+version = "0.2.4"
 edition = "2021"
 license = "MIT"
 description = "A Rust-based embedded operating system designed to enable memory safe, memory efficient, reliable, and responsive applications."
@@ -9,7 +9,7 @@ readme = "README.md"
 categories = ["embedded", "no-std"]
 
 [features]
-default = ["unwind", "offload_unwind"]
+default = ["unwind"]
 # Enable unwinding to clean up panicked tasks.
 unwind = ["dep:gimli", "dep:fallible-iterator"]
 # Print the program counter (PC) of each function in the unwound stack.
@@ -33,10 +33,6 @@ stm32f469 = ["hopter_proc_macro/stm32f469", "stm32f4xx-hal/stm32f469"]
 # Used for testing with GitHub workflow and locally with QEMU.
 qemu = ["stm32f405"]
 
-# Used to test offloading the stack unwinder 
-offload_unwind = []
-offload_debug = ["offload_unwind"]
-
 [dependencies]
 static_assertions = "1.1"
 heapless = "0.8"
@@ -45,7 +41,6 @@ int-enum = "1.1"
 cortex-m-semihosting = "0.5"
 concat-idents = "1.0"
 paste = "1.0"
-hadusos = "0.2.1"
 
 [dependencies.stable_deref_trait]
 version = "1.2.0"
@@ -53,7 +48,7 @@ default-features = false
 features = ["alloc"]
 
 [dependencies.hopter_proc_macro]
-version = "0.2.0"
+version = "0.2.2"
 
 [dependencies.hopter_conf_params]
 version = "0.2.0"
@@ -84,7 +79,7 @@ features = ["nightly"]
 [dev-dependencies]
 nb = "1.1"
 
-[dependencies.stm32f4xx-hal]
+[dev-dependencies.stm32f4xx-hal]
 version = "0.21.0"
 default-features = false
 
diff --git a/examples/off_unw.rs b/examples/off_unw.rs
deleted file mode 100644
index dce3bf0..0000000
--- a/examples/off_unw.rs
+++ /dev/null
@@ -1,109 +0,0 @@
-#![no_std]
-#![no_main]
-#![feature(naked_functions)]
-
-extern crate alloc;
-use core::sync::atomic::{AtomicUsize, Ordering};
-use hadusos::Session;
-// use hopter::time::sleep_ms;
-use hopter::{
-    debug::semihosting::{self, dbg_println},
-    task::{self, main},
-    time::sleep_ms,
-    uart::{UsartSerial, UsartTimer, G_UART_MAILBOX, G_UART_RBYTE, G_UART_RX, G_UART_SESSION},
-};
-use hopter_proc_macro::handler;
-use stm32f4xx_hal::serial::{Rx, Tx};
-use stm32f4xx_hal::uart::Config;
-use stm32f4xx_hal::{pac::USART1, prelude::*};
-// Attribute `#[main]` marks the function as the entry function for the main
-// task. The function name can be arbitrary. The main function should accept
-// one argument which is the Cortex-M core peripherals.
-#[main]
-fn main(_: cortex_m::Peripherals) {
-    dbg_println!("Beginning unw_iter example: Initializing global hadusos session");
-
-    // Initialize the hadusos Session with the UART peripheral.
-    // First we acquire the peripherals for the tx and rx pins
-    let dp = unsafe { stm32f4xx_hal::pac::Peripherals::steal() };
-    let clocks = dp.RCC.constrain().cfgr.freeze();
-    let gpioa = dp.GPIOA.split();
-
-    let usart1_pins = (
-        gpioa.pa9.into_alternate::<7>(),
-        gpioa.pa10.into_alternate::<7>(),
-    );
-    let mut rx: Rx<USART1>;
-    let tx: Tx<USART1>;
-    (tx, rx) = dp
-        .USART1
-        .serial(
-            usart1_pins,
-            Config::default().baudrate(115200.bps()),
-            &clocks,
-        )
-        .unwrap()
-        .split();
-
-    rx.listen();
-
-    unsafe {
-        G_UART_RX = Some(rx);
-    }
-
-    unsafe { cortex_m::peripheral::NVIC::unmask(stm32f4xx_hal::pac::Interrupt::USART1) };
-
-    let usart_serial = UsartSerial { tx };
-    let usart_timer = UsartTimer {};
-    let session: Session<UsartSerial, UsartTimer, 150, 2> = Session::new(usart_serial, usart_timer);
-
-    unsafe { G_UART_SESSION = Some(session) };
-
-    // Start a task running the `will_panic` function.
-    // The task is restartable. When the panic occurs, the task's stack will be
-    // unwound, and the task will be restarted.
-    task::build()
-        .set_entry(will_panic)
-        .spawn_restartable()
-        .unwrap();
-}
-
-fn will_panic() {
-    // A persistent counter.
-    static CNT: AtomicUsize = AtomicUsize::new(0);
-
-    // Every time the task runs we increment it by 1.
-    let cnt = CNT.fetch_add(1, Ordering::SeqCst);
-
-    dbg_println!("Current count: {}", cnt);
-
-    // Panic and get restarted for 5 times.
-    if cnt == 0 {
-        dbg_println!("Panic now!");
-        panic!();
-    }
-    let _ = sleep_ms(35000);
-    dbg_println!("Finished");
-
-    // When running with QEMU, this will cause the QEMU process to terminate.
-    // Do not include this line when running with OpenOCD, because it will
-    // clobber its internal states.
-    #[cfg(feature = "qemu")]
-    semihosting::terminate(true);
-    #[cfg(not(feature = "qemu"))]
-    {
-        dbg_println!("test complete!");
-        loop {}
-    }
-}
-
-#[handler(USART1)]
-fn usart1_handler() {
-    cortex_m::interrupt::free(|_| {
-        unsafe {
-            let _ = G_UART_RBYTE.push_back(G_UART_RX.as_mut().unwrap().read().unwrap());
-        };
-        // Notify the mailbox that a byte is available to read by incrementing the counter
-        G_UART_MAILBOX.notify_allow_isr();
-    });
-}
diff --git a/examples/tests/debug/cpu_load/load_40_percent.rs b/examples/tests/debug/cpu_load/load_40_percent.rs
index c14b22a..80e8f3c 100644
--- a/examples/tests/debug/cpu_load/load_40_percent.rs
+++ b/examples/tests/debug/cpu_load/load_40_percent.rs
@@ -3,6 +3,7 @@
 #![no_main]
 #![no_std]
 #![feature(naked_functions)]
+#![feature(asm_const)]
 
 extern crate alloc;
 
diff --git a/examples/tests/interrupt/unwind/simple.rs b/examples/tests/interrupt/unwind/simple.rs
index d05a442..65e9585 100644
--- a/examples/tests/interrupt/unwind/simple.rs
+++ b/examples/tests/interrupt/unwind/simple.rs
@@ -3,6 +3,7 @@
 #![no_main]
 #![no_std]
 #![feature(naked_functions)]
+#![feature(asm_const)]
 
 extern crate alloc;
 
diff --git a/examples/tests/sync/channel/try_consume_from_isr.rs b/examples/tests/sync/channel/try_consume_from_isr.rs
index c6f9320..ff50641 100644
--- a/examples/tests/sync/channel/try_consume_from_isr.rs
+++ b/examples/tests/sync/channel/try_consume_from_isr.rs
@@ -3,6 +3,7 @@
 #![no_main]
 #![no_std]
 #![feature(naked_functions)]
+#![feature(asm_const)]
 
 extern crate alloc;
 
diff --git a/examples/tests/sync/channel/try_produce_from_isr.rs b/examples/tests/sync/channel/try_produce_from_isr.rs
index 8f2160a..38f2540 100644
--- a/examples/tests/sync/channel/try_produce_from_isr.rs
+++ b/examples/tests/sync/channel/try_produce_from_isr.rs
@@ -4,6 +4,7 @@
 #![no_main]
 #![no_std]
 #![feature(naked_functions)]
+#![feature(asm_const)]
 
 extern crate alloc;
 
diff --git a/examples/tests/sync/mailbox/notify_from_isr.rs b/examples/tests/sync/mailbox/notify_from_isr.rs
index 1e1cccb..b1ae76b 100644
--- a/examples/tests/sync/mailbox/notify_from_isr.rs
+++ b/examples/tests/sync/mailbox/notify_from_isr.rs
@@ -3,6 +3,7 @@
 #![no_main]
 #![no_std]
 #![feature(naked_functions)]
+#![feature(asm_const)]
 
 extern crate alloc;
 
diff --git a/examples/tests/sync/semaphore/try_down_from_isr.rs b/examples/tests/sync/semaphore/try_down_from_isr.rs
index 6c549f9..ab17e4e 100644
--- a/examples/tests/sync/semaphore/try_down_from_isr.rs
+++ b/examples/tests/sync/semaphore/try_down_from_isr.rs
@@ -3,6 +3,7 @@
 #![no_main]
 #![no_std]
 #![feature(naked_functions)]
+#![feature(asm_const)]
 
 extern crate alloc;
 
diff --git a/examples/tests/sync/semaphore/try_up_from_isr.rs b/examples/tests/sync/semaphore/try_up_from_isr.rs
index 49c301b..d775808 100644
--- a/examples/tests/sync/semaphore/try_up_from_isr.rs
+++ b/examples/tests/sync/semaphore/try_up_from_isr.rs
@@ -3,6 +3,7 @@
 #![no_main]
 #![no_std]
 #![feature(naked_functions)]
+#![feature(asm_const)]
 
 extern crate alloc;
 
diff --git a/examples/tests/task/context_switch/fp_registers.rs b/examples/tests/task/context_switch/fp_registers.rs
index a684328..fd2195b 100644
--- a/examples/tests/task/context_switch/fp_registers.rs
+++ b/examples/tests/task/context_switch/fp_registers.rs
@@ -13,10 +13,10 @@ use hopter::{
     task::main,
 };
 
-/// Whether the verifier task is running.
-static TEST_STARTED: AtomicBool = AtomicBool::new(false);
+/// Whether the clobbering task should run.
+static RUN_CLOBBER: AtomicBool = AtomicBool::new(false);
 
-/// Whether the cloberring task has executed.
+/// Whether the cloberring task has run.
 static CLOBBERED: AtomicBool = AtomicBool::new(false);
 
 static mut KNOWN_VALUE: [f32; 32] = [
@@ -48,44 +48,75 @@ fn main(_: cortex_m::Peripherals) {
 extern "C" fn verify_registers() -> ! {
     unsafe {
         asm!(
-            // Set `TEST_STARTED` to true.
-            "ldr    r0, ={test_started}",
+            // Set `RUN_CLOBBER` to true.
+            "0:",
+            "ldr    r0, ={run_clobber}",
             "mov    r1, #1",
             "strb   r1, [r0]",
-            "0:",
             // Set register `s0-s15` to known values.
             "ldr    r0, ={known_value}",
             "vldmia r0, {{s0-s15}}",
             // Trigger context switch.
-            "svc    #1",
+            "mov    r0, #0xe0",
+            "msr    basepri, r0",
+            "mov    r1, #0x10000000",
+            "movw   r0, #0xed04",
+            "movt   r0, #0xe000",
+            "str    r1, [r0]",
+            "mov    r0, #0",
+            "msr    basepri, r0",
+            // See if the clobbering task has run.
+            "ldr    r0, ={clobber}",
+            "ldrb   r0, [r0]",
+            // If the clobbering task has not run yet, we loop back and do
+            // everything another time.
+            "cmp    r0, #0",
+            "beq    0b",
             // Examine the values of registers `s0-s15`. They should remain the
             // same as before the context switch.
             "ldr    r0, ={known_value}",
             "vldmia r0, {{s16-s31}}",
             "bl     {compare_fp_regs}",
+            "1:",
+            // Set `CLOBERRED` to false.
+            "ldr    r0, ={cloberred}",
+            "mov    r1, #1",
+            "strb   r1, [r0]",
+            // Set `RUN_CLOBBER` to true.
+            "ldr    r0, ={run_clobber}",
+            "mov    r1, #1",
+            "strb   r1, [r0]",
             // Set register `s16-s31` to known values.
             "ldr    r0, ={known_value} + 64",
             "vldmia r0, {{s16-s31}}",
             // Trigger context switch.
-            "svc    #1",
-            // Examine the values of registers `s16-s31`. They should remain the
-            // same as before the context switch.
-            "ldr    r0, ={known_value} + 64",
-            "vldmia r0, {{s0-s15}}",
-            "bl     {compare_fp_regs}",
+            "mov    r0, #0xe0",
+            "msr    basepri, r0",
+            "mov    r1, #0x10000000",
+            "movw   r0, #0xed04",
+            "movt   r0, #0xe000",
+            "str    r1, [r0]",
+            "mov    r0, #0",
+            "msr    basepri, r0",
             // See if the clobbering task has run.
             "ldr    r0, ={clobber}",
             "ldrb   r0, [r0]",
             // If the clobbering task has not run yet, we loop back and do
             // everything another time.
             "cmp    r0, #0",
-            "beq    0b",
+            "beq    1b",
+            // Examine the values of registers `s16-s31`. They should remain the
+            // same as before the context switch.
+            "ldr    r0, ={known_value} + 64",
+            "vldmia r0, {{s0-s15}}",
+            "bl     {compare_fp_regs}",
             // If the clobbering task has run, then we have verified that the
             // registers in this task's context were not affected. Declare
             // success.
             "b     {success}",
-            test_started = sym TEST_STARTED,
+            run_clobber = sym RUN_CLOBBER,
             known_value = sym KNOWN_VALUE,
+            cloberred = sym CLOBBERED,
             compare_fp_regs = sym compare_fp_regs,
             clobber = sym clobber_all_fp_regs,
             success = sym success,
@@ -100,15 +131,22 @@ extern "C" fn verify_registers() -> ! {
 extern "C" fn clobber_all_fp_regs() -> ! {
     unsafe {
         asm!(
-            "ldr    r0, ={test_started}",
             "0:",
-            // Load the current value of `TEST_STARTED`.
+            "ldr    r0, ={run_clobber}",
+            // Load the current value of `RUN_CLOBBER`.
             "ldrb   r1, [r0]",
             "cmp    r1, #0",
             // Goto cloberring the register if has started.
             "bne    1f",
             // Otherwise, perform a context switch and try again.
-            "svc    #1",
+            "mov    r0, #0xe0",
+            "msr    basepri, r0",
+            "mov    r1, #0x10000000",
+            "movw   r0, #0xed04",
+            "movt   r0, #0xe000",
+            "str    r1, [r0]",
+            "mov    r0, #0",
+            "msr    basepri, r0",
             "b      0b",
             // The verify task is running now. Clobber all registers. This
             // should not affect the registers in the verify task's context.
@@ -120,12 +158,22 @@ extern "C" fn clobber_all_fp_regs() -> ! {
             // Clobber registers.
             "ldr    r0, ={clobbered_value}",
             "vldmia r0, {{s0-s31}}",
+            // Set `RUN_CLOBBER` to false.
+            "ldr    r0, ={run_clobber}",
+            "mov    r1, #1",
+            "strb   r1, [r0]",
             // Perform context switch so that the verifier task can perform
             // the check.
-            "2:",
-            "svc    #1",
-            "b      2b",
-            test_started = sym TEST_STARTED,
+            "mov    r0, #0xe0",
+            "msr    basepri, r0",
+            "mov    r1, #0x10000000",
+            "movw   r0, #0xed04",
+            "movt   r0, #0xe000",
+            "str    r1, [r0]",
+            "mov    r0, #0",
+            "msr    basepri, r0",
+            "b      0b",
+            run_clobber = sym RUN_CLOBBER,
             clobbered_value = sym CLOBBERED_VALUE,
             cloberred = sym CLOBBERED,
             options(noreturn)
diff --git a/examples/tests/task/context_switch/gp_registers.rs b/examples/tests/task/context_switch/gp_registers.rs
index 9474c72..49042fc 100644
--- a/examples/tests/task/context_switch/gp_registers.rs
+++ b/examples/tests/task/context_switch/gp_registers.rs
@@ -13,10 +13,10 @@ use hopter::{
     task::main,
 };
 
-/// Whether the verifier task is running.
-static TEST_STARTED: AtomicBool = AtomicBool::new(false);
+/// Whether the clobbering task should run.
+static RUN_CLOBBER: AtomicBool = AtomicBool::new(false);
 
-/// Whether the cloberring task has executed.
+/// Whether the cloberring task has run.
 static CLOBBERED: AtomicBool = AtomicBool::new(false);
 
 #[main]
@@ -41,15 +41,12 @@ fn main(_: cortex_m::Peripherals) {
 extern "C" fn verify_registers() -> ! {
     unsafe {
         asm!(
-            // Set `TEST_STARTED` to true.
-            "ldr  r0, ={test_started}",
+            // Set `RUN_CLOBBER` to true.
+            "0:",
+            "ldr  r0, ={run_clobber}",
             "mov  r1, #1",
             "strb r1, [r0]",
-            "0:",
-            // Preserve the current stack pointer value in the stack.
-            "mov  r0, sp",
-            "push {{r0}}",
-            // Write some known values to the registers.
+            // Write some known values to the low registers.
             "mov  r0, #1",
             "mov  r1, #2",
             "mov  r2, #3",
@@ -58,16 +55,24 @@ extern "C" fn verify_registers() -> ! {
             "mov  r5, #6",
             "mov  r6, #7",
             "mov  r7, #8",
-            "mov  r8, #9",
-            "mov  r9, #10",
-            "mov  r10, #11",
-            "mov  r11, #12",
-            "mov  r12, #13",
-            "mov  lr, #14",
             // Trigger context switch.
-            "svc  #1",
-            // Examine the values of registers. They should remain the same as
-            // before the context switch.
+            "mov  r8, #0xe0",
+            "msr  basepri, r8",
+            "mov  r9, #0x10000000",
+            "movw r8, #0xed04",
+            "movt r8, #0xe000",
+            "str  r9, [r8]",
+            "mov  r8, #0",
+            "msr  basepri, r8",
+            // See if the clobbering task has run.
+            "ldr  r8, ={clobber}",
+            "ldrb r8, [r8]",
+            // If the clobbering task has not run yet, we loop back and do
+            // everything another time.
+            "cmp  r8, #0",
+            "beq  0b",
+            // Examine the values of low registers. They should remain the same
+            // as before the context switch.
             "cmp  r0, #1",
             "bne  {error}",
             "cmp  r1, #2",
@@ -84,6 +89,43 @@ extern "C" fn verify_registers() -> ! {
             "bne  {error}",
             "cmp  r7, #8",
             "bne  {error}",
+            "1:",
+            // Set `CLOBERRED` to false.
+            "ldr  r0, ={cloberred}",
+            "mov  r1, #1",
+            "strb r1, [r0]",
+            // Set `RUN_CLOBBER` to true.
+            "ldr  r0, ={run_clobber}",
+            "mov  r1, #1",
+            "strb r1, [r0]",
+            // Preserve the current stack pointer value in the stack.
+            "mov  r0, sp",
+            "push {{r0}}",
+            // Write some known values to the high registers.
+            "mov  r8, #9",
+            "mov  r9, #10",
+            "mov  r10, #11",
+            "mov  r11, #12",
+            "mov  r12, #13",
+            "mov  lr, #14",
+            // Trigger context switch.
+            "mov  r0, #0xe0",
+            "msr  basepri, r0",
+            "mov  r1, #0x10000000",
+            "movw r0, #0xed04",
+            "movt r0, #0xe000",
+            "str  r1, [r0]",
+            "mov  r0, #0",
+            "msr  basepri, r0",
+            // See if the clobbering task has run.
+            "ldr  r0, ={clobber}",
+            "ldrb r0, [r0]",
+            // If the clobbering task has not run yet, we loop back and do
+            // everything another time.
+            "cmp  r0, #0",
+            "beq  1b",
+            // Examine the values of high registers. They should remain the same
+            // as before the context switch.
             "cmp  r8, #9",
             "bne  {error}",
             "cmp  r9, #10",
@@ -101,18 +143,12 @@ extern "C" fn verify_registers() -> ! {
             "pop  {{r0}}",
             "cmp  r0, sp",
             "bne  {error}",
-            // See if the clobbering task has run.
-            "ldr  r0, ={clobber}",
-            "ldrb r0, [r0]",
-            // If the clobbering task has not run yet, we loop back and do
-            // everything another time.
-            "cmp  r0, #0",
-            "beq  0b",
             // If the clobbering task has run, then we have verified that the
             // registers in this task's context were not affected. Declare
             // success.
             "b   {success}",
-            test_started = sym TEST_STARTED,
+            run_clobber = sym RUN_CLOBBER,
+            cloberred = sym CLOBBERED,
             clobber = sym clobber_all_gp_regs,
             error = sym error,
             success = sym success,
@@ -127,15 +163,22 @@ extern "C" fn verify_registers() -> ! {
 extern "C" fn clobber_all_gp_regs() -> ! {
     unsafe {
         asm!(
-            "ldr  r0, ={test_started}",
             "0:",
-            // Load the current value of `TEST_STARTED`.
+            "ldr  r0, ={run_clobber}",
+            // Load the current value of `RUN_CLOBBER`.
             "ldrb r1, [r0]",
             "cmp  r1, #0",
             // Goto cloberring the register if has started.
             "bne  1f",
             // Otherwise, perform a context switch and try again.
-            "svc  #1",
+            "mov  r0, #0xe0",
+            "msr  basepri, r0",
+            "mov  r1, #0x10000000",
+            "movw r0, #0xed04",
+            "movt r0, #0xe000",
+            "str  r1, [r0]",
+            "mov  r0, #0",
+            "msr  basepri, r0",
             "b    0b",
             // The verify task is running now. Clobber all registers. This
             // should not affect the registers in the verify task's context.
@@ -159,12 +202,22 @@ extern "C" fn clobber_all_gp_regs() -> ! {
             "mov  r11, #0xffffffff",
             "mov  r12, #0xffffffff",
             "mov  lr, #0xffffffff",
+            // Set `RUN_CLOBBER` to false.
+            "ldr  r0, ={run_clobber}",
+            "mov  r1, #1",
+            "strb r1, [r0]",
             // Perform context switch so that the verifier task can perform
             // the check.
-            "2:",
-            "svc  #1",
-            "b    2b",
-            test_started = sym TEST_STARTED,
+            "mov  r0, #0xe0",
+            "msr  basepri, r0",
+            "mov  r1, #0x10000000",
+            "movw r0, #0xed04",
+            "movt r0, #0xe000",
+            "str  r1, [r0]",
+            "mov  r0, #0",
+            "msr  basepri, r0",
+            "b    0b",
+            run_clobber = sym RUN_CLOBBER,
             cloberred = sym CLOBBERED,
             options(noreturn)
         )
diff --git a/src/boot/system_init.rs b/src/boot/system_init.rs
index a129653..b204a2f 100644
--- a/src/boot/system_init.rs
+++ b/src/boot/system_init.rs
@@ -10,7 +10,7 @@ use cortex_m::peripheral::scb::SystemHandler;
 pub(super) extern "C" fn system_start() -> ! {
     allocator::initialize();
 
-    let mut cp = cortex_m::Peripherals::take().unwrap();
+    let mut cp = unsafe { cortex_m::Peripherals::steal() };
 
     // Configure system call and context switch exception priority.
     unsafe {
@@ -68,14 +68,14 @@ fn enable_systick(cp: &mut cortex_m::Peripherals) {
     cp.SYST.enable_interrupt();
 }
 
-extern "C" {
+extern "Rust" {
     /// A glue function that calls to the user defined main function with
-    /// the [`#[main]`](super::main) attribute.
+    /// the [`#[main]`](crate::task::main) attribute.
     fn __main_trampoline(arg: AtomicPtr<u8>);
 }
 
 /// The first non-idle task run by the scheduler. It enables SysTick and then
-/// calls the user defined main function with the [`#[main]`](super::main)
+/// calls the user defined main function with the [`#[main]`](crate::task::main)
 /// attribute.
 fn main_task(mut cp: cortex_m::Peripherals) {
     enable_systick(&mut cp);
diff --git a/src/interrupt/context_switch.rs b/src/interrupt/context_switch.rs
index f4e4018..5926c2a 100644
--- a/src/interrupt/context_switch.rs
+++ b/src/interrupt/context_switch.rs
@@ -112,3 +112,16 @@ extern "C" fn pendsv_handler(ex_ret_lr: u32) {
     // chosen task to run.
     Scheduler::pick_next();
 }
+
+/// Invoke the scheduler to choose a new task to run.
+///
+/// A task may voluntarily yield the CPU or it may be forced to yield, e.g.,
+/// when becoming blocked on a synchronization primitive.
+///
+/// This function should be called in a task's context and never in an ISR's
+/// context.
+pub(crate) fn yield_current_task() {
+    cortex_m::peripheral::SCB::set_pendsv();
+    cortex_m::asm::dsb();
+    cortex_m::asm::isb();
+}
diff --git a/src/interrupt/entry_exit.rs b/src/interrupt/entry_exit.rs
deleted file mode 100644
index 40c1159..0000000
--- a/src/interrupt/entry_exit.rs
+++ /dev/null
@@ -1,74 +0,0 @@
-use crate::config;
-use core::arch::asm;
-
-extern "C" fn handler_trampoline(handler_func_ptr: u32) {
-    #[cfg(feature = "unwind")]
-    use crate::unwind;
-
-    #[cfg(feature = "unwind")]
-    let saved_is_handler_unwinding = unwind::unwind::save_and_clear_isr_unwinding();
-
-    // Following the documentation, we `as`-cast to a raw pointer before
-    // `transmute`ing to a function pointer. This avoids an integer-to-pointer
-    // `transmute`, which can be problematic. Transmuting between raw pointers
-    // and function pointers (i.e., two pointer types) is fine.
-    let handler_func_ptr = handler_func_ptr as *const ();
-    let handler_func = unsafe { core::mem::transmute::<*const (), fn()>(handler_func_ptr) };
-
-    #[cfg(not(feature = "unwind"))]
-    handler_func();
-
-    #[cfg(feature = "unwind")]
-    {
-        let _ = unwind::unw_catch::catch_unwind(|| handler_func());
-        unwind::unwind::set_isr_unwinding(saved_is_handler_unwinding);
-    }
-}
-
-/// Entry point for servicing an IRQ, which will never do context switch.
-/// After the IRQ is serviced, it will return back to the currently running task.
-/// Caller-saved registers are already pushed onto the task's stack by the
-/// hardware. Callee-saved registers are preserved by the handler functions
-/// if they need to use them.
-#[naked]
-pub unsafe extern "C" fn entry(handler_func_ptr: u32) {
-    asm!(
-        // Preserve the task local storage (TLS) fields and exception return
-        // value.
-        "ldr   r12, ={tls_mem_addr}",
-        "ldmia r12, {{r1-r3}}",
-        "push  {{r1-r3, lr}}",
-        // Set the kernel stacklet boundary and clear out other fields in the
-        // TLS.
-        "ldr   r1, ={kern_stk_boundary}",
-        "mov   r2, #0",
-        "mov   r3, #0",
-        "stmia r12, {{r1-r3}}",
-        // Let the handler return to the exit sequence.
-        "ldr   lr, ={exit}",
-        // Run the IRQ handler.
-        "b     {handler_trampoline}",
-        tls_mem_addr = const config::__TLS_MEM_ADDR,
-        kern_stk_boundary = const config::__CONTIGUOUS_STACK_BOUNDARY,
-        exit = sym exit,
-        handler_trampoline = sym handler_trampoline,
-        options(noreturn)
-    )
-}
-
-/// Exception return back to the currently running task. Assuming that the
-/// system is running with FPU and the task has floating point context.
-#[naked]
-unsafe extern "C" fn exit() {
-    asm!(
-        // Restore the task local storage (TLS) fields and exception return
-        // value.
-        "pop   {{r1-r3, lr}}",
-        "ldr   r12, ={tls_mem_addr}",
-        "stmia r12, {{r1-r3}}",
-        // Exception return.
-        "bx    lr",
-        tls_mem_addr = const config::__TLS_MEM_ADDR,
-        options(noreturn)
-    )
-}
diff --git a/src/interrupt/mod.rs b/src/interrupt/mod.rs
index ce1a035..d07e266 100644
--- a/src/interrupt/mod.rs
+++ b/src/interrupt/mod.rs
@@ -7,6 +7,4 @@ pub(crate) mod svc_handler;
 pub(crate) mod trap_frame;
 
 pub mod declare;
-#[doc(hidden)]
-pub mod entry_exit;
 pub mod mask;
diff --git a/src/interrupt/svc.rs b/src/interrupt/svc.rs
index 12621b5..9b6c912 100644
--- a/src/interrupt/svc.rs
+++ b/src/interrupt/svc.rs
@@ -41,21 +41,6 @@ pub(crate) unsafe extern "C" fn svc_free(ptr: *mut u8) {
     )
 }
 
-/// Yield the current task. Let the scheduler choose the next task to run.
-/// A task may voluntarily yield the CPU or it may be forced to yield when
-/// becoming blocked on a synchronization primitive.
-#[naked]
-pub(crate) extern "C" fn svc_yield_current_task() {
-    unsafe {
-        asm!(
-            "svc {task_yield}",
-            "bx lr",
-            task_yield = const(SVCNum::TaskYield as u8),
-            options(noreturn)
-        )
-    }
-}
-
 /// Terminate the current task and free its task struct.
 #[naked]
 pub(crate) unsafe extern "C" fn svc_destroy_current_task() {
diff --git a/src/interrupt/svc_handler.rs b/src/interrupt/svc_handler.rs
index 0a1d5a1..ddf67f8 100644
--- a/src/interrupt/svc_handler.rs
+++ b/src/interrupt/svc_handler.rs
@@ -1,17 +1,12 @@
 //! A task invokes SVC when the kernel operation requested by the task needs
 //! to run with the kernel contiguous stack. An SVC always returns back to the
-//! calling task, i.e., SVC itself does not *directly* perform any context
-//! switch. Based on this invariant, the SVC entry instruction sequence is
-//! optimized so that only minimal context is saved. Specifically, caller-saved
-//! registers are pushed to the user segmented stack, forming the trap frame,
-//! while callee-saved registers are preserved by the handler functions
-//! following the function call ABI. So, there is no need to make a copy of
-//! callee-saved registers, unlike in [`TaskCtxt`](crate::task::TaskCtxt).
-//!
-//! To clarify, a task indeed invokes SVC to yield, but the actual context
-//! switch is done by chaining PendSV after the SVC. Logically, the SVC still
-//! returns to the yielding task, but PendSV then immediately causes the task
-//! to be switched out of the CPU.
+//! calling task, i.e., SVC does not perform any context switch. Based on this
+//! invariant, the SVC entry instruction sequence is optimized so that only
+//! minimal context is saved. Specifically, caller-saved registers are pushed
+//! to the user segmented stack, forming the trap frame, while callee-saved
+//! registers are preserved by the handler functions following the function
+//! call ABI. So, there is no need to make a copy of callee-saved registers,
+//! unlike in [`TaskCtxt`](crate::task::TaskCtxt).
 
 use super::trap_frame::TrapFrame;
 use crate::{
@@ -26,21 +21,14 @@ use int_enum::IntEnum;
 #[repr(u8)]
 #[derive(IntEnum)]
 pub(crate) enum SVCNum {
-    /// The calling task wants to get off from CPU. A task may voluntarily
-    /// yield the CPU or it may be forced to yield when becoming blocked on a
-    /// synchronization primitive.
-    ///
-    /// Note that the SVC handler does not directly perform a context switch.
-    /// Instead, a PendSV will be tail chained to perform it.
-    TaskYield = 1,
     /// The task wants to terminate and release its task struct.
-    TaskDestroy = 2,
+    TaskDestroy = 0,
     /// The calling task wants to release its top stacklet.
-    TaskLessStack = 3,
+    TaskLessStack = 1,
     /// The task wants to allocate dynamic memory.
-    MemAlloc = 4,
+    MemAlloc = 2,
     /// The task wants to free dynamic memory.
-    MemFree = 5,
+    MemFree = 3,
     /// The task wants to allocate a stacklet to run the stack unwinder.
     TaskUnwindPrepare = 252,
     /// The task wants to release the stacklet used to run the unwinder and
@@ -147,7 +135,6 @@ fn get_svc_num(tf: &TrapFrame) -> SVCNum {
 /// the assembly code.
 extern "C" fn svc_handler(tf: &mut TrapFrame, ctxt: &mut TaskSVCCtxt) {
     match get_svc_num(tf) {
-        SVCNum::TaskYield => Scheduler::yield_current_task_from_svc(),
         SVCNum::TaskDestroy => Scheduler::drop_current_task_from_svc(),
         SVCNum::TaskLessStack => task::less_stack(tf, ctxt),
         SVCNum::TaskMoreStack => task::more_stack(tf, ctxt, MoreStackReason::Normal),
diff --git a/src/interrupt/systick.rs b/src/interrupt/systick.rs
index b52a481..b3edaff 100644
--- a/src/interrupt/systick.rs
+++ b/src/interrupt/systick.rs
@@ -1,14 +1,29 @@
+use crate::{config, time};
 use core::arch::asm;
 
-/// Entry function for handling SysTick interrupt.
-/// Prepare the handler in the r0 register and start the IRQ handling.
 #[naked]
 #[export_name = "SysTick"]
 unsafe extern "C" fn systick_entry() {
     asm!(
-        "ldr r0, ={systick_handler}",
-        "b {entry}",
-        entry = sym super::entry_exit::entry,
+        // Preserve the task local storage (TLS) fields and exception return value.
+        "ldr   r0, ={tls_mem_addr}",
+        "ldmia r0, {{r1-r3}}",
+        "push  {{r1-r3, lr}}",
+        // Set the kernel stacklet boundary and clear out other fields in the TLS.
+        "ldr   r1, ={cont_stk_boundary}",
+        "mov   r2, #0",
+        "strd  r1, r2, [r0]",
+        "str   r2, [r0, #8]",
+        // Run the IRQ handler.
+        "bl    {systick_handler}",
+        // Restore the task local storage (TLS) fields and exception return value.
+        "pop   {{r1-r3}}",
+        "ldr   r0, ={tls_mem_addr}",
+        "stmia r0, {{r1-r3}}",
+        // Exception return.
+        "pop   {{pc}}",
+        tls_mem_addr = const config::__TLS_MEM_ADDR,
+        cont_stk_boundary = const config::__CONTIGUOUS_STACK_BOUNDARY,
         systick_handler = sym systick_handler,
         options(noreturn)
     )
@@ -16,7 +31,6 @@ unsafe extern "C" fn systick_entry() {
 
 /// We simply need to advance the tick count when SysTick fires.
 unsafe extern "C" fn systick_handler() {
-    use crate::time;
     time::advance_tick();
     time::wake_sleeping_tasks();
 }
diff --git a/src/lib.rs b/src/lib.rs
index ab50c89..e225768 100644
--- a/src/lib.rs
+++ b/src/lib.rs
@@ -76,7 +76,6 @@ mod assembly;
 mod boot;
 mod schedule;
 mod unrecoverable;
-mod unwind;
 
 pub mod config;
 pub mod debug;
@@ -84,4 +83,6 @@ pub mod interrupt;
 pub mod sync;
 pub mod task;
 pub mod time;
-pub mod uart;
\ No newline at end of file
+
+#[doc(hidden)]
+pub mod unwind;
diff --git a/src/schedule/current.rs b/src/schedule/current.rs
index 50c39d0..e50c4c2 100644
--- a/src/schedule/current.rs
+++ b/src/schedule/current.rs
@@ -1,4 +1,4 @@
-use super::scheduler::Scheduler;
+use super::scheduler::{SchedSuspendGuard, Scheduler};
 use crate::{
     sync::{RwSpin, RwSpinReadGuard, RwSpinWriteGuard},
     task::{Task, TaskCtxt},
@@ -59,44 +59,84 @@ pub(super) fn update_cur_task(task: Arc<Task>) {
 #[no_mangle]
 pub(crate) static CUR_TASK_CTXT_PTR: AtomicPtr<TaskCtxt> = AtomicPtr::new(core::ptr::null_mut());
 
-/// Do things with the current task struct. When the given closure is being
-/// executed, the current task `Arc` will be locked in reader mode and no
-/// context switch will happen during this period.
+/// Execute the given closure with the current task struct as the argument. The
+/// current task struct is provided as an `Arc<Task>`. When the given closure
+/// is being executed, no context switch will happen during this period, and
+/// the [`CUR_TASK`] will be locked in reader mode.
 ///
-/// [`with_current_task_arc`] has slightly better performance than this
-/// function. Use that function if `&Task` suffices.
-pub(crate) fn with_current_task_arc<F, R>(closure: F) -> R
+/// [`with_cur_task`] has slightly better performance than this function.
+/// Use that function if `Arc<Task>` is an overkill and `&Task` suffices.
+///
+/// If before calling this function, the scheduler has already been suspended
+/// and a [`SchedSuspendGuard`] is available, call
+/// [`with_cur_task_arc_explicit_sched_suspend`] for better performance by
+/// avoiding recursively suspending the scheduler.
+pub(crate) fn with_cur_task_arc<F, R>(op: F) -> R
 where
     F: FnOnce(Arc<Task>) -> R,
+{
+    let guard = Scheduler::suspend();
+    with_cur_task_arc_explicit_sched_suspend(&guard, op)
+}
+
+/// Execute the given closure with the current task struct as the argument. The
+/// current task struct is provided as a `&Task`. When the given closure is
+/// being executed, no context switch will happen during this period, and
+/// the [`CUR_TASK`] will be locked in reader mode.
+///
+/// This function has slightly better performance than [`with_cur_task_arc`].
+///
+/// If before calling this function, the scheduler has already been suspended
+/// and a [`SchedSuspendGuard`] is available, call
+/// [`with_cur_task_explicit_sched_suspend`] for better performance by
+/// avoiding recursively suspending the scheduler.
+pub(crate) fn with_cur_task<F, R>(op: F) -> R
+where
+    F: FnOnce(&Task) -> R,
 {
     // Suspend the scheduler and lock the current task `Arc` in reader mode.
-    let _sched_suspend_guard = Scheduler::suspend();
-    let read_guard = CUR_TASK.read();
+    let guard = Scheduler::suspend();
+    with_cur_task_explicit_sched_suspend(&guard, op)
+}
+
+/// Execute the given closure with the current task struct as the argument. The
+/// scheduler must be suspended beforehand, proven by passing a
+/// [`SchedSuspendGuard`] to this function. The current task struct is provided
+/// as an `Arc<Task>`. When the given closure is being executed the [`CUR_TASK`]
+/// will be locked in reader mode.
+///
+/// [`with_cur_task_explicit_sched_suspend`] has slightly better performance
+/// than this function. Use that function if `Arc<Task>` is an overkill and
+/// `&Task` suffices.
+pub(crate) fn with_cur_task_arc_explicit_sched_suspend<F, R>(_guard: &SchedSuspendGuard, op: F) -> R
+where
+    F: FnOnce(Arc<Task>) -> R,
+{
+    let read_guard: RwSpinReadGuard<_> = CUR_TASK.read();
 
-    // Run the closure.
     if let Some(cur_task) = &*read_guard {
-        closure(cur_task.clone())
+        op(cur_task.clone())
     } else {
         unrecoverable::die();
     }
 }
 
-/// Do things with the current task struct. When the given closure is being
-/// executed, the current task `Arc` will be locked in reader mode and no
-/// context switch will happen during this period.
+/// Execute the given closure with the current task struct as the argument. The
+/// scheduler must be suspended beforehand, proven by passing a
+/// [`SchedSuspendGuard`] to this function. The current task struct is provided
+/// as an `Arc<Task>`. When the given closure is being executed the [`CUR_TASK`]
+/// will be locked in reader mode.
 ///
-/// This function has slightly better performance than [`with_current_task_arc`].
-pub(crate) fn with_current_task<F, R>(closure: F) -> R
+/// This function has slightly better performance than
+/// [`with_cur_task_arc_explicit_sched_suspend`].
+pub(crate) fn with_cur_task_explicit_sched_suspend<F, R>(_guard: &SchedSuspendGuard, op: F) -> R
 where
     F: FnOnce(&Task) -> R,
 {
-    // Suspend the scheduler and lock the current task `Arc` in reader mode.
-    let _sched_suspend_guard = Scheduler::suspend();
     let read_guard: RwSpinReadGuard<_> = CUR_TASK.read();
 
-    // Run the closure.
     if let Some(cur_task) = &*read_guard {
-        closure(cur_task)
+        op(cur_task)
     } else {
         unrecoverable::die();
     }
diff --git a/src/schedule/idle.rs b/src/schedule/idle.rs
index e7ff8e8..0b2b9e3 100644
--- a/src/schedule/idle.rs
+++ b/src/schedule/idle.rs
@@ -1,5 +1,5 @@
 use crate::{
-    interrupt::svc,
+    interrupt::context_switch,
     sync::{SpinSchedSafe, SpinSchedSafeGuard},
 };
 use alloc::{sync::Arc, vec::Vec};
@@ -28,7 +28,7 @@ pub(super) unsafe extern "C" fn idle_task() -> ! {
     // The idle task is always executed first when the scheduler is just
     // started. A main task should always be present awaiting to run. Perform
     // a context switch to let the main task run.
-    svc::svc_yield_current_task();
+    context_switch::yield_current_task();
 
     // If nothing to do, enter low power state.
     loop {
diff --git a/src/schedule/scheduler.rs b/src/schedule/scheduler.rs
index 7ed59aa..b6cf9c0 100644
--- a/src/schedule/scheduler.rs
+++ b/src/schedule/scheduler.rs
@@ -1,7 +1,7 @@
 use super::{current, idle};
 use crate::{
     config,
-    interrupt::svc,
+    interrupt::context_switch,
     sync::{Access, AllowPendOp, Holdable, RefCellSchedSafe, RunPendedOp, SoftLock, Spin},
     task::{Task, TaskListAdapter, TaskListInterfaces, TaskState},
     unrecoverable::{self, Lethal},
@@ -59,7 +59,7 @@ struct InnerPendAccessor<'a> {
 /// them linked.
 impl<'a> RunPendedOp for InnerFullAccessor<'a> {
     fn run_pended_op(&mut self) {
-        current::with_current_task(|cur_task| {
+        current::with_cur_task(|cur_task| {
             let mut locked_list = self.ready_linked_list.lock_now_or_die();
             while let Some(task) = self.insert_buffer.dequeue() {
                 if task.should_preempt(cur_task) {
@@ -182,13 +182,12 @@ impl Scheduler {
             unrecoverable::die();
         }
 
-        READY_TASK_QUEUE
-            .lock()
-            .must_with_full_access(|full_access| {
+        READY_TASK_QUEUE.with_suspended_scheduler(|queue, sched_guard| {
+            queue.must_with_full_access(|full_access| {
                 let mut locked_list = full_access.ready_linked_list.lock_now_or_die();
 
                 // Clean up for the current task.
-                current::with_current_task_arc(|cur_task| {
+                current::with_cur_task_arc_explicit_sched_suspend(sched_guard, |cur_task| {
                     match cur_task.get_state() {
                         // Put the current task back to the ready queue only if the
                         // task is in `Running` state.
@@ -260,6 +259,7 @@ impl Scheduler {
                 // performed one.
                 PENDING_CTXT_SWITCH.store(false, Ordering::SeqCst);
             })
+        })
     }
 
     /// Return if the scheduler has been started.
@@ -281,34 +281,36 @@ impl Scheduler {
 
     /// Internal implementation to insert a task to the ready queue.
     fn insert_task_to_ready_queue(task: Arc<Task>) {
-        READY_TASK_QUEUE.lock().with_access(|access| match access {
-            // The queue is not under contention. Directly put the task to the
-            // linked list.
-            Access::Full { full_access } => {
-                // Request a context switch if the incoming ready task has a
-                // higher priority than the current task. Check it only when
-                // the scheduler has started otherwise there will be no current
-                // task.
-                if Scheduler::has_started() {
-                    current::with_current_task(|cur_task| {
-                        if task.should_preempt(cur_task) {
-                            PENDING_CTXT_SWITCH.store(true, Ordering::SeqCst);
-                        }
-                    });
-                }
+        READY_TASK_QUEUE.with_suspended_scheduler(|queue, sched_guard| {
+            queue.with_access(|access| match access {
+                // The queue is not under contention. Directly put the task to the
+                // linked list.
+                Access::Full { full_access } => {
+                    // Request a context switch if the incoming ready task has a
+                    // higher priority than the current task. Check it only when
+                    // the scheduler has started otherwise there will be no current
+                    // task.
+                    if Scheduler::has_started() {
+                        current::with_cur_task_explicit_sched_suspend(sched_guard, |cur_task| {
+                            if task.should_preempt(cur_task) {
+                                PENDING_CTXT_SWITCH.store(true, Ordering::SeqCst);
+                            }
+                        });
+                    }
 
-                // Put the ready task to the linked list.
-                task.set_state(TaskState::Ready);
-                let mut locked_list = full_access.ready_linked_list.lock_now_or_die();
-                locked_list.push_back(task);
-            }
-            // The queue is under contention. The current execution context, which
-            // must be an ISR, preempted another context that is holding the full
-            // access. Place the task in the lock-free buffer. The full access
-            // holder will later put it back to the linked list.
-            Access::PendOnly { pend_access } => {
-                pend_access.insert_buffer.enqueue(task).unwrap_or_die();
-            }
+                    // Put the ready task to the linked list.
+                    task.set_state(TaskState::Ready);
+                    let mut locked_list = full_access.ready_linked_list.lock_now_or_die();
+                    locked_list.push_back(task);
+                }
+                // The queue is under contention. The current execution context, which
+                // must be an ISR, preempted another context that is holding the full
+                // access. Place the task in the lock-free buffer. The full access
+                // holder will later put it back to the linked list.
+                Access::PendOnly { pend_access } => {
+                    pend_access.insert_buffer.enqueue(task).unwrap_or_die();
+                }
+            })
         });
     }
 
@@ -328,7 +330,7 @@ impl Scheduler {
             // Go through an SVC to perform context switch if currently is in
             // task context.
             if current::is_in_task_context() {
-                svc::svc_yield_current_task();
+                context_switch::yield_current_task();
             // Tail chain a PendSV to directly perform a context switch if
             // currently is in an ISR context. But if the code is already
             // *performing* context switch, i.e., called by PendSV, then we
@@ -349,14 +351,8 @@ impl Scheduler {
     pub(crate) fn drop_current_task_from_svc() {
         // Mark the task state as `Destructing` so that the scheduler will drop
         // the task struct upon a later context switch.
-        current::with_current_task(|cur_task| cur_task.set_state(TaskState::Destructing));
-
-        // Tail chain a PendSV to perform a context switch.
-        cortex_m::peripheral::SCB::set_pendsv()
-    }
+        current::with_cur_task(|cur_task| cur_task.set_state(TaskState::Destructing));
 
-    /// Switch to another ready task.
-    pub(crate) fn yield_current_task_from_svc() {
         // Tail chain a PendSV to perform a context switch.
         cortex_m::peripheral::SCB::set_pendsv()
     }
diff --git a/src/sync/mailbox.rs b/src/sync/mailbox.rs
index ed7fee9..394483c 100644
--- a/src/sync/mailbox.rs
+++ b/src/sync/mailbox.rs
@@ -1,7 +1,7 @@
 use super::{Access, AllowPendOp, RefCellSchedSafe, RunPendedOp, SoftLock, Spin};
 use crate::{
-    interrupt::svc,
-    schedule::current,
+    interrupt::context_switch,
+    schedule::{current, scheduler::Scheduler},
     task::{Task, TaskState},
     time, unrecoverable,
 };
@@ -40,18 +40,44 @@ struct Inner {
     pending_count: AtomicUsize,
     /// The task waiting on this [`Mailbox`]. The spin lock around it is only
     /// for sanity check. This field should not be accessed concurrently.
-    wait_task: Spin<Option<Arc<Task>>>,
+    wait_task: Spin<WaitTask>,
     /// Whether the [`notify_allow_isr`](Mailbox::notify_allow_isr) has been
     /// invoked. This is used to distinguish between waking up a task by
-    /// notification and by timeout.
+    /// notification and by timeout. The field is meaningful only when the
+    /// waiting task is [`WithTimeout`](WaitTask::WithTimeout).
     task_notified: AtomicBool,
 }
 
+/// Enumaration of the waiting task category.
+enum WaitTask {
+    /// The task is waiting with a timeout.
+    ///
+    /// IMPORTANT: In this case, the sleeping queue in [`crate::time`] is
+    /// logically holding the ownership of the task. When the task is woken
+    /// up either due to notification or timeout, it is the `Arc<Task>` inside
+    /// the sleeping queue that will be put back to the scheduler's ready queue.
+    /// Logically, it is better to use `Weak<Task>` with this enum variant, but
+    /// we choose to still use `Arc<Task>` to reduce code size bloat. This is
+    /// in contrast with the [`WithoutTimeout`](WaitTask::WithoutTimeout)
+    /// variant.
+    WithTimeout(Arc<Task>),
+    /// The task is waiting without a specified timeout.
+    ///
+    /// IMPORTANT: In this case, the logical ownership of the waiting task is
+    /// maintained by this enum variant. When the task is woken up, it is the
+    /// `Arc<Task>` carried by this enum variant that will be put back to the
+    /// scheduler's ready queue. This is in contrast with the
+    /// [`WithTimeout`](WaitTask::WithTimeout) variant.
+    WithoutTimeout(Arc<Task>),
+    /// No task is waiting on the mailbox.
+    NoTask,
+}
+
 /// Representing full access to all fields of the [`Mailbox`].
 struct InnerFullAccessor<'a> {
     count: &'a AtomicUsize,
     pending_count: &'a AtomicUsize,
-    wait_task: &'a Spin<Option<Arc<Task>>>,
+    wait_task: &'a Spin<WaitTask>,
     task_notified: &'a AtomicBool,
 }
 
@@ -101,10 +127,24 @@ impl<'a> RunPendedOp for InnerFullAccessor<'a> {
         // incremented to be greater than zero. (See `notify_allow_isr`.) It
         // follows that now `count` must also be greater than zero. Thus, as
         // long as there is a waiting task, we should notify it.
-        if let Some(wait_task) = self.wait_task.lock_now_or_die().take() {
-            time::remove_task_from_sleep_queue_allow_isr(wait_task);
-            self.count.fetch_sub(1, Ordering::SeqCst);
-            self.task_notified.store(true, Ordering::SeqCst);
+        match self.wait_task.lock_now_or_die().take() {
+            // If there is a waiting task with timeout, wake it up. The task's
+            // ownership is moved from the sleeping queue to the scheduler's
+            // ready queue. See the documentation of `WithTimeout` for details.
+            WaitTask::WithTimeout(wait_task) => {
+                time::remove_task_from_sleep_queue_allow_isr(wait_task);
+                self.count.fetch_sub(1, Ordering::SeqCst);
+                self.task_notified.store(true, Ordering::SeqCst);
+            }
+            // If there is a waiting task without timeout, wake it up. The
+            // task's ownership is moved from this enum variant to the
+            // scheduler's ready queue. See the documentation of
+            // `WithoutTimeout` for details.
+            WaitTask::WithoutTimeout(wait_task) => {
+                Scheduler::accept_task(wait_task);
+                self.count.fetch_sub(1, Ordering::SeqCst);
+            }
+            WaitTask::NoTask => {}
         }
     }
 }
@@ -114,7 +154,7 @@ impl Inner {
         Self {
             count: AtomicUsize::new(0),
             pending_count: AtomicUsize::new(0),
-            wait_task: Spin::new(None),
+            wait_task: Spin::new(WaitTask::NoTask),
             task_notified: AtomicBool::new(false),
         }
     }
@@ -138,10 +178,39 @@ impl Mailbox {
     ///
     /// NOTE: *must not* call this method in ISR context.
     pub fn wait(&self) {
-        // Just wait with a very large timeout. This has very little overhead
-        // on scheduling. Continue to wait until the task is woken up by a
-        // notification rather than a timeout.
-        while !self.wait_until_timeout(100_000_000) {}
+        unrecoverable::die_if_in_isr();
+
+        let mut should_block = true;
+
+        // Suspend scheduling and acquire full access to the mailbox fields.
+        self.inner.with_suspended_scheduler(|mailbox, sched_guard| {
+            mailbox.must_with_full_access(|full_access| {
+                let mut locked_wait_task = full_access.wait_task.lock_now_or_die();
+
+                // A sanity check to prevent more than one task to try to wait on
+                // the same mailbox.
+                assert!(locked_wait_task.is_no_task());
+
+                // If the counter is currently positive, decrement the counter and
+                // do not block.
+                if full_access.count.load(Ordering::SeqCst) > 0 {
+                    full_access.count.fetch_sub(1, Ordering::SeqCst);
+                    should_block = false;
+                    return;
+                }
+
+                current::with_cur_task_arc_explicit_sched_suspend(sched_guard, |cur_task| {
+                    cur_task.set_state(TaskState::Blocked);
+
+                    // Record the waiting task on this mailbox.
+                    *locked_wait_task = WaitTask::WithoutTimeout(Arc::clone(&cur_task));
+                });
+            })
+        });
+
+        if should_block {
+            context_switch::yield_current_task();
+        }
     }
 
     /// Block the calling task if the notification counter is currently zero.
@@ -166,52 +235,56 @@ impl Mailbox {
         let mut should_block = true;
 
         // Suspend scheduling and acquire full access to the mailbox fields.
-        self.inner.lock().must_with_full_access(|full_access| {
-            let mut locked_wait_task = full_access.wait_task.lock_now_or_die();
-
-            // A sanity check to prevent more than one task to try to wait on
-            // the same mailbox.
-            assert!(locked_wait_task.is_none());
-
-            // If the counter is currently positive, decrement the counter and
-            // do not block.
-            if full_access.count.load(Ordering::SeqCst) > 0 {
-                full_access.count.fetch_sub(1, Ordering::SeqCst);
-                should_block = false;
-                return;
-            }
+        self.inner.with_suspended_scheduler(|mailbox, sched_guard| {
+            mailbox.must_with_full_access(|full_access| {
+                let mut locked_wait_task = full_access.wait_task.lock_now_or_die();
 
-            // Otherwise the task is going to be blocked. Reset the flag.
-            full_access.task_notified.store(false, Ordering::SeqCst);
+                // A sanity check to prevent more than one task to try to wait on
+                // the same mailbox.
+                assert!(locked_wait_task.is_no_task());
 
-            current::with_current_task_arc(|cur_task| {
-                cur_task.set_state(TaskState::Blocked);
+                // If the counter is currently positive, decrement the counter and
+                // do not block.
+                if full_access.count.load(Ordering::SeqCst) > 0 {
+                    full_access.count.fetch_sub(1, Ordering::SeqCst);
+                    should_block = false;
+                    return;
+                }
 
-                // Record the waiting task on this mailbox.
-                *locked_wait_task = Some(Arc::clone(&cur_task));
+                // Otherwise the task is going to be blocked. Reset the flag.
+                full_access.task_notified.store(false, Ordering::SeqCst);
 
-                // Add the waiting task to the sleeping queue.
-                // FIXME: This assumes 1ms tick interval.
-                let wake_at_tick = time::get_tick() + timeout_ms;
-                time::add_task_to_sleep_queue(cur_task, wake_at_tick);
-            });
+                current::with_cur_task_arc_explicit_sched_suspend(sched_guard, |cur_task| {
+                    cur_task.set_state(TaskState::Blocked);
+
+                    // Record the waiting task on this mailbox.
+                    *locked_wait_task = WaitTask::WithTimeout(Arc::clone(&cur_task));
+
+                    // Add the waiting task to the sleeping queue.
+                    // FIXME: This assumes 1ms tick interval.
+                    let wake_at_tick = time::get_tick() + timeout_ms;
+                    time::add_task_to_sleep_queue(cur_task, wake_at_tick);
+                });
+            })
         });
 
         if should_block {
             // If the task should block, request a context switch.
-            svc::svc_yield_current_task();
+            context_switch::yield_current_task();
 
             // We reach here if either the waiting task is notified or the
             // waiting time reaches timeout.
 
             // Suspend scheduling and acquire full access to the mailbox fields.
-            self.inner.lock().must_with_full_access(|full_access| {
-                // Clear the waiting task field. This field was not cleared if
-                // the task wakes up because of the timeout.
-                full_access.wait_task.lock_now_or_die().take();
+            self.inner.with_suspended_scheduler(|mailbox, _| {
+                mailbox.must_with_full_access(|full_access| {
+                    // Clear the waiting task field. This field was not cleared if
+                    // the task wakes up because of the timeout.
+                    full_access.wait_task.lock_now_or_die().take();
 
-                // Return whether the task wakes up because of notification.
-                full_access.task_notified.load(Ordering::SeqCst)
+                    // Return whether the task wakes up because of notification.
+                    full_access.task_notified.load(Ordering::SeqCst)
+                })
             })
         } else {
             // If the task need not block, it consumed a notification count and
@@ -227,28 +300,58 @@ impl Mailbox {
     /// This method is allowed in ISR context.
     pub fn notify_allow_isr(&self) {
         // Suspend scheduling and get access to the mailbox fields.
-        self.inner.lock().with_access(|access| match access {
-            // If we have full access to the inner fields, we directly wake up
-            // the waiting task or increment the counter.
-            Access::Full { full_access } => match full_access.wait_task.lock_now_or_die().take() {
-                // If there is a waiting task, wake it up.
-                Some(wait_task) => {
-                    time::remove_task_from_sleep_queue_allow_isr(wait_task);
-                    full_access.task_notified.store(true, Ordering::SeqCst);
+        self.inner.with_suspended_scheduler(|mailbox, _| {
+            mailbox.with_access(|access| match access {
+                // If we have full access to the inner fields, we directly wake up
+                // the waiting task or increment the counter.
+                Access::Full { full_access } => {
+                    match full_access.wait_task.lock_now_or_die().take() {
+                        // If there is a waiting task with timeout, wake it up. The
+                        // task's ownership is moved from the sleeping queue to the
+                        // scheduler's ready queue. See the documentation of
+                        // `WithTimeout` for details.
+                        WaitTask::WithTimeout(wait_task) => {
+                            time::remove_task_from_sleep_queue_allow_isr(wait_task);
+                            full_access.task_notified.store(true, Ordering::SeqCst);
+                        }
+                        // If there is a waiting task without timeout, wake it up. The
+                        // task's ownership is moved from this enum variant to the
+                        // scheduler's ready queue. See the documentation of
+                        // `WithoutTimeout` for details.
+                        WaitTask::WithoutTimeout(wait_task) => {
+                            Scheduler::accept_task(wait_task);
+                        }
+                        // If there is not a waiting task, increment the counter.
+                        WaitTask::NoTask => {
+                            full_access.count.fetch_add(1, Ordering::SeqCst);
+                        }
+                    }
                 }
-                // If there is not a waiting task, increment the counter.
-                None => {
-                    full_access.count.fetch_add(1, Ordering::SeqCst);
-                    full_access.task_notified.store(true, Ordering::SeqCst);
+                // If other context is running with the full access and we preempt
+                // it, we get pend-only access. We increment the `pending_count` so
+                // that the full access owner can later help us update the counter
+                // or notify the waiting task on behalf.
+                Access::PendOnly { pend_access } => {
+                    pend_access.pending_count.fetch_add(1, Ordering::SeqCst);
                 }
-            },
-            // If other context is running with the full access and we preempt
-            // it, we get pend-only access. We increment the `pending_count` so
-            // that the full access owner can later help us update the counter
-            // or notify the waiting task on behalf.
-            Access::PendOnly { pend_access } => {
-                pend_access.pending_count.fetch_add(1, Ordering::SeqCst);
-            }
+            })
         });
     }
 }
+
+impl WaitTask {
+    /// Similar to [`Option::take`], return the current value and replace it
+    /// with [`WaitTask::NoTask`].
+    fn take(&mut self) -> Self {
+        core::mem::replace(self, Self::NoTask)
+    }
+
+    /// Return if the variant is [`WaitTask::NoTask`].
+    fn is_no_task(&self) -> bool {
+        if let Self::NoTask = self {
+            true
+        } else {
+            false
+        }
+    }
+}
diff --git a/src/sync/mutex.rs b/src/sync/mutex.rs
index a55dfd0..9da43e8 100644
--- a/src/sync/mutex.rs
+++ b/src/sync/mutex.rs
@@ -122,7 +122,7 @@ where
             // task pointer.
             .and_then(|guard| {
                 if !current::is_in_isr_context() {
-                    current::with_current_task_arc(|cur_task| {
+                    current::with_cur_task_arc(|cur_task| {
                         self.owner.lock_now_or_die().replace(cur_task)
                     });
                 }
@@ -153,7 +153,7 @@ where
         }
 
         // Priority inheritance.
-        current::with_current_task(|cur_task| {
+        current::with_cur_task(|cur_task| {
             let locked_owner = self.owner.lock_now_or_die();
             if let Some(owner) = locked_owner.as_ref() {
                 owner.ceil_priority_from(cur_task);
diff --git a/src/sync/refcell_sched_safe.rs b/src/sync/refcell_sched_safe.rs
index dcd63cb..e45dfa8 100644
--- a/src/sync/refcell_sched_safe.rs
+++ b/src/sync/refcell_sched_safe.rs
@@ -1,6 +1,7 @@
 use crate::schedule::scheduler::{SchedSuspendGuard, Scheduler};
-use core::ops::Deref;
 
+/// A lock type that grants access to the contained data when the scheduler
+/// is suspended.
 pub(crate) struct RefCellSchedSafe<T>
 where
     T: ?Sized,
@@ -9,28 +10,20 @@ where
 }
 
 impl<T> RefCellSchedSafe<T> {
-    pub const fn new(val: T) -> Self {
+    /// Create a new [`RefCellSchedSafe`] instance wrapping the given value.
+    pub(crate) const fn new(val: T) -> Self {
         Self { val }
     }
 
-    pub fn lock(&self) -> RefSchedSafe<T> {
-        let _guard = Scheduler::suspend();
-        RefSchedSafe {
-            val_ref: &self.val,
-            _guard,
-        }
-    }
-}
-
-pub(crate) struct RefSchedSafe<'a, T> {
-    val_ref: &'a T,
-    _guard: SchedSuspendGuard,
-}
-
-impl<'a, T> Deref for RefSchedSafe<'a, T> {
-    type Target = T;
-
-    fn deref(&self) -> &Self::Target {
-        &self.val_ref
+    /// Suspend the scheduler and run the given closure. The closure should
+    /// take two arguments: `&T` and [`&SchedSuspendGuard`](SchedSuspendGuard).
+    /// The granted access is not `mut` because even when the scheduler is
+    /// suspended, an interrupt handler can still concurrently access the data.
+    pub(crate) fn with_suspended_scheduler<F, R>(&self, op: F) -> R
+    where
+        F: FnOnce(&T, &SchedSuspendGuard) -> R,
+    {
+        let guard = Scheduler::suspend();
+        op(&self.val, &guard)
     }
 }
diff --git a/src/sync/wait_queue.rs b/src/sync/wait_queue.rs
index 526e170..40c10a4 100644
--- a/src/sync/wait_queue.rs
+++ b/src/sync/wait_queue.rs
@@ -2,7 +2,7 @@ use super::{
     Access, AllowPendOp, Lockable, RefCellSchedSafe, RunPendedOp, SoftLock, Spin, UnlockableGuard,
 };
 use crate::{
-    interrupt::svc,
+    interrupt::context_switch,
     schedule::{current, scheduler::Scheduler},
     task::{TaskListAdapter, TaskListInterfaces, TaskState},
     unrecoverable,
@@ -103,19 +103,21 @@ impl WaitQueue {
 
         // We have put the current task to the wait queue.
         // Tell the scheduler to run another task.
-        svc::svc_yield_current_task();
+        context_switch::yield_current_task();
 
         // Outline the logic to reduce the stack frame size of `.wait()`.
         #[inline(never)]
         fn add_cur_task_to_block_queue(wq: &WaitQueue) {
             // Should always grant full access to a task.
-            wq.inner.lock().must_with_full_access(|full_access| {
-                // Put the current task into the queue.
-                current::with_current_task_arc(|cur_task| {
-                    cur_task.set_state(TaskState::Blocked);
-                    let mut locked_queue = full_access.queue.lock_now_or_die();
-                    locked_queue.push_back(cur_task);
-                });
+            wq.inner.with_suspended_scheduler(|queue, sched_guard| {
+                queue.must_with_full_access(|full_access| {
+                    // Put the current task into the queue.
+                    current::with_cur_task_arc_explicit_sched_suspend(sched_guard, |cur_task| {
+                        cur_task.set_state(TaskState::Blocked);
+                        let mut locked_queue = full_access.queue.lock_now_or_die();
+                        locked_queue.push_back(cur_task);
+                    });
+                })
             });
         }
     }
@@ -151,7 +153,7 @@ impl WaitQueue {
 
             // Otherwise, we have put the current task to the wait queue.
             // Tell the scheduler to run another task.
-            svc::svc_yield_current_task();
+            context_switch::yield_current_task();
         }
 
         // Outline the logic to reduce the stack frame size of `.wait_until()`.
@@ -164,24 +166,26 @@ impl WaitQueue {
             F: FnMut() -> Option<R>,
         {
             // Should always grant full access to a task.
-            wq.inner.lock().must_with_full_access(|full_access| {
-                // Must lock the queue here before evaluating the condition to
-                // prevent deadlock.
-                let mut locked_queue = full_access.queue.lock_now_or_die();
+            wq.inner.with_suspended_scheduler(|queue, sched_guard| {
+                queue.must_with_full_access(|full_access| {
+                    // Must lock the queue here before evaluating the condition to
+                    // prevent deadlock.
+                    let mut locked_queue = full_access.queue.lock_now_or_die();
 
-                // Check if the predicate is satisfied and if yes return the value
-                // contained in `Some`.
-                if let Some(ret) = condition() {
-                    return Some(ret);
-                }
+                    // Check if the predicate is satisfied and if yes return the value
+                    // contained in `Some`.
+                    if let Some(ret) = condition() {
+                        return Some(ret);
+                    }
 
-                // Otherwise, put the current task into the queue.
-                current::with_current_task_arc(|cur_task| {
-                    cur_task.set_state(TaskState::Blocked);
-                    locked_queue.push_back(cur_task);
-                });
+                    // Otherwise, put the current task into the queue.
+                    current::with_cur_task_arc_explicit_sched_suspend(sched_guard, |cur_task| {
+                        cur_task.set_state(TaskState::Blocked);
+                        locked_queue.push_back(cur_task);
+                    });
 
-                None
+                    None
+                })
             })
         }
     }
@@ -226,7 +230,7 @@ impl WaitQueue {
                 // the scheduler to run another task. After this task is scheduled
                 // again, take back the lock and try again.
                 Ok(mutex) => {
-                    svc::svc_yield_current_task();
+                    context_switch::yield_current_task();
                     guard = mutex.lock_and_get_guard();
                 }
             }
@@ -245,27 +249,29 @@ impl WaitQueue {
             L: Lockable<GuardType<'a> = G> + 'a,
         {
             // Should always grant full access to a task.
-            wq.inner.lock().must_with_full_access(|full_access| {
-                // Must lock the queue here before evaluating the condition and
-                // releasing the `guard` passed in argument to prevent deadlock.
-                let mut locked_queue = full_access.queue.lock_now_or_die();
+            wq.inner.with_suspended_scheduler(|queue, sched_guard| {
+                queue.must_with_full_access(|full_access| {
+                    // Must lock the queue here before evaluating the condition and
+                    // releasing the `guard` passed in argument to prevent deadlock.
+                    let mut locked_queue = full_access.queue.lock_now_or_die();
 
-                // Check if the predicate is satisfied and if yes return the value
-                // contained in `Some` with the lock guard.
-                if let Some(ret) = condition(&mut guard) {
-                    return Err((guard, ret));
-                }
+                    // Check if the predicate is satisfied and if yes return the value
+                    // contained in `Some` with the lock guard.
+                    if let Some(ret) = condition(&mut guard) {
+                        return Err((guard, ret));
+                    }
 
-                // Otherwise, release the lock guard and get the lock itself.
-                let mutex = guard.unlock_and_into_lock_ref();
+                    // Otherwise, release the lock guard and get the lock itself.
+                    let mutex = guard.unlock_and_into_lock_ref();
 
-                // Put the current task into the queue.
-                current::with_current_task_arc(|cur_task| {
-                    cur_task.set_state(TaskState::Blocked);
-                    locked_queue.push_back(cur_task);
-                });
+                    // Put the current task into the queue.
+                    current::with_cur_task_arc_explicit_sched_suspend(sched_guard, |cur_task| {
+                        cur_task.set_state(TaskState::Blocked);
+                        locked_queue.push_back(cur_task);
+                    });
 
-                Ok(mutex)
+                    Ok(mutex)
+                })
             })
         }
     }
@@ -280,21 +286,23 @@ impl WaitQueue {
     /// will be in turn notified, i.e., the notification is treated as spurious and
     /// is discarded.
     pub(super) fn notify_one_allow_isr(&self) {
-        self.inner.lock().with_access(|access| match access {
-            // If we have full access to the inner components, we directly operate
-            // on the queue to make the popped task ready.
-            Access::Full { full_access } => {
-                let mut locked_queue = full_access.queue.lock_now_or_die();
-                if let Some(task) = locked_queue.pop_highest_priority() {
-                    Scheduler::accept_task(task);
+        self.inner.with_suspended_scheduler(|queue, _| {
+            queue.with_access(|access| match access {
+                // If we have full access to the inner components, we directly operate
+                // on the queue to make the popped task ready.
+                Access::Full { full_access } => {
+                    let mut locked_queue = full_access.queue.lock_now_or_die();
+                    if let Some(task) = locked_queue.pop_highest_priority() {
+                        Scheduler::accept_task(task);
+                    }
                 }
-            }
-            // If other context is running with the full access and we preempt it,
-            // we get pend-only access. We increment the counter so that the full
-            // access owner can later pop out the task on our behalf.
-            Access::PendOnly { pend_access } => {
-                pend_access.notify_cnt.fetch_add(1, Ordering::SeqCst);
-            }
+                // If other context is running with the full access and we preempt it,
+                // we get pend-only access. We increment the counter so that the full
+                // access owner can later pop out the task on our behalf.
+                Access::PendOnly { pend_access } => {
+                    pend_access.notify_cnt.fetch_add(1, Ordering::SeqCst);
+                }
+            })
         });
     }
 }
diff --git a/src/task/current.rs b/src/task/current.rs
index 175efaa..559e7e1 100644
--- a/src/task/current.rs
+++ b/src/task/current.rs
@@ -1,6 +1,6 @@
 use crate::{
     config,
-    interrupt::svc,
+    interrupt::context_switch,
     schedule::{current, scheduler::Scheduler},
 };
 
@@ -9,7 +9,7 @@ use crate::{
 pub fn yield_current() {
     // Yield only if the scheduler is not suspended.
     if !Scheduler::is_suspended() {
-        svc::svc_yield_current_task();
+        context_switch::yield_current_task();
     }
 }
 
@@ -24,13 +24,13 @@ pub fn change_current_priority(prio: u8) -> Result<(), ()> {
     if prio >= config::TASK_PRIORITY_LEVELS - 1 {
         return Err(());
     }
-    current::with_current_task(|cur_task| cur_task.change_intrinsic_priority(prio));
-    svc::svc_yield_current_task();
+    current::with_cur_task(|cur_task| cur_task.change_intrinsic_priority(prio));
+    context_switch::yield_current_task();
     Ok(())
 }
 
 /// Return the ID of the current task. The ID is only for diagnostic purpose
 /// and does not have any functional purpose.
 pub fn get_current_id() -> u8 {
-    current::with_current_task(|cur_task| cur_task.get_id())
+    current::with_cur_task(|cur_task| cur_task.get_id())
 }
diff --git a/src/task/segmented_stack.rs b/src/task/segmented_stack.rs
index b5a3a90..b8f3b02 100644
--- a/src/task/segmented_stack.rs
+++ b/src/task/segmented_stack.rs
@@ -265,7 +265,7 @@ pub(crate) fn more_stack(tf: &mut TrapFrame, ctxt: &mut TaskSVCCtxt, reason: Mor
     #[cfg(not(feature = "unwind"))]
     let abort = false;
 
-    current::with_current_task(|cur_task| {
+    current::with_cur_task(|cur_task| {
         // Define a closure to be invoked when the stack size limit is
         // exceeded.
         #[cfg(feature = "unwind")]
@@ -456,7 +456,7 @@ pub(crate) fn less_stack(tf: &TrapFrame, ctxt: &mut TaskSVCCtxt) {
         ctxt.tls.stklet_bound = meta.prev_stklet_bound;
         ctxt.sp = meta.prev_sp;
 
-        current::with_current_task(|cur_task| {
+        current::with_cur_task(|cur_task| {
             cur_task.with_stack_ctrl_block(|scb| {
                 // Update hot split alleviation information.
                 svc_less_stack_anti_hot_split(prev_tf, scb);
diff --git a/src/task/trampoline.rs b/src/task/trampoline.rs
index 3149acc..434e24b 100644
--- a/src/task/trampoline.rs
+++ b/src/task/trampoline.rs
@@ -65,7 +65,7 @@ where
         // If the task panicked, check if it has already been restarted with
         // another task struct. If yes, we break the loop to let the current
         // task struct terminates.
-        if current::with_current_task(|cur_task| cur_task.has_restarted()) {
+        if current::with_cur_task(|cur_task| cur_task.has_restarted()) {
             break;
         }
 
@@ -86,7 +86,7 @@ where
         // The following catch is to detect such pathological case. If it
         // happens, give up restarting the task.
         if let Err(_) = unw_catch::catch_unwind(|| {
-            current::with_current_task(|cur_task| cur_task.set_unwind_flag(false))
+            current::with_cur_task(|cur_task| cur_task.set_unwind_flag(false))
         }) {
             break;
         }
diff --git a/src/time/mod.rs b/src/time/mod.rs
index f12641a..1c9aa30 100644
--- a/src/time/mod.rs
+++ b/src/time/mod.rs
@@ -1,6 +1,6 @@
 use crate::{
     config,
-    interrupt::svc,
+    interrupt::context_switch,
     schedule::{current, scheduler::Scheduler},
     sync::{Access, AllowPendOp, RefCellSchedSafe, RunPendedOp, SoftLock, Spin},
     task::{Task, TaskListAdapter, TaskListInterfaces, TaskState},
@@ -116,13 +116,15 @@ pub fn get_tick() -> u32 {
 
 /// Wake up those sleeping tasks that have their sleeping time expired.
 pub(crate) fn wake_sleeping_tasks() {
-    SLEEP_TASK_QUEUE.lock().with_access(|access| match access {
-        Access::Full { full_access } => {
-            full_access.wake_expired_tasks();
-        }
-        Access::PendOnly { pend_access } => {
-            pend_access.time_to_wakeup.store(true, Ordering::SeqCst)
-        }
+    SLEEP_TASK_QUEUE.with_suspended_scheduler(|queue, _| {
+        queue.with_access(|access| match access {
+            Access::Full { full_access } => {
+                full_access.wake_expired_tasks();
+            }
+            Access::PendOnly { pend_access } => {
+                pend_access.time_to_wakeup.store(true, Ordering::SeqCst)
+            }
+        })
     });
 }
 
@@ -148,13 +150,13 @@ fn sleep_ms_unchecked(ms: u32) {
 
         // Yield from the current task. Even if the current task has already
         // been woken up, yielding from it will not introduce deadlock.
-        svc::svc_yield_current_task();
+        context_switch::yield_current_task();
     }
 
     // Outline the logic to reduce the stack frame size of `sleep_ms`.
     #[inline(never)]
     fn add_cur_task_to_sleep_queue(wake_at_tick: u32) {
-        current::with_current_task_arc(|cur_task| {
+        current::with_cur_task_arc(|cur_task| {
             cur_task.set_state(TaskState::Blocked);
             add_task_to_sleep_queue(cur_task, wake_at_tick);
         })
@@ -162,26 +164,28 @@ fn sleep_ms_unchecked(ms: u32) {
 }
 
 pub(crate) fn add_task_to_sleep_queue(task: Arc<Task>, wake_at_tick: u32) {
-    SLEEP_TASK_QUEUE
-        .lock()
-        .must_with_full_access(|full_access| {
+    SLEEP_TASK_QUEUE.with_suspended_scheduler(|queue, _| {
+        queue.must_with_full_access(|full_access| {
             task.set_wake_tick(wake_at_tick);
             let mut locked_queue = full_access.time_sorted_queue.lock_now_or_die();
             locked_queue.push_back_tick_sorted(task);
-        });
+        })
+    });
 }
 
 pub(crate) fn remove_task_from_sleep_queue_allow_isr(task: Arc<Task>) {
-    SLEEP_TASK_QUEUE.lock().with_access(|access| match access {
-        Access::Full { full_access } => {
-            let mut locked_queue = full_access.time_sorted_queue.lock_now_or_die();
-            if let Some(task) = locked_queue.remove_task(&task) {
-                Scheduler::accept_task(task);
+    SLEEP_TASK_QUEUE.with_suspended_scheduler(|queue, _| {
+        queue.with_access(|access| match access {
+            Access::Full { full_access } => {
+                let mut locked_queue = full_access.time_sorted_queue.lock_now_or_die();
+                if let Some(task) = locked_queue.remove_task(&task) {
+                    Scheduler::accept_task(task);
+                }
             }
-        }
-        Access::PendOnly { pend_access } => {
-            pend_access.delete_buffer.enqueue(task).unwrap_or_die();
-        }
+            Access::PendOnly { pend_access } => {
+                pend_access.delete_buffer.enqueue(task).unwrap_or_die();
+            }
+        })
     });
 }
 
diff --git a/src/uart/mod.rs b/src/uart/mod.rs
deleted file mode 100644
index a67f09d..0000000
--- a/src/uart/mod.rs
+++ /dev/null
@@ -1,67 +0,0 @@
-// #![feature(naked_functions)]
-extern crate alloc;
-
-// use crate::interrupt::handler;
-
-use crate::{sync::Mailbox, time::get_tick};
-use core::cmp::max;
-use hadusos::{Serial, SerialError, Session, Timer};
-use stm32f4xx_hal::{
-    pac::USART1,
-    prelude::*,
-    serial::{Rx, Tx},
-};
-
-pub static mut G_UART_SESSION: Option<Session<UsartSerial, UsartTimer, 150, 2>> = None;
-
-pub static G_UART_MAILBOX: Mailbox = Mailbox::new();
-pub static mut G_UART_RX: Option<Rx<USART1>> = None;
-pub static mut G_UART_MAX_SIZE: usize = 0;
-pub static mut G_UART_RBYTE: heapless::Deque<u8, 128> = heapless::Deque::new();
-pub static G_TIMEOUT_MS: u32 = 30000;
-
-#[derive(Debug)]
-pub enum UartError {
-    ReadError,
-    WriteError,
-    Uninitialized,
-    Timeout,
-}
-
-pub struct UsartTimer {}
-
-impl Timer for UsartTimer {
-    fn get_timestamp_ms(&mut self) -> u32 {
-        let tick = get_tick();
-        tick
-    }
-}
-
-pub struct UsartSerial {
-    pub tx: Tx<USART1>,
-}
-impl Serial for UsartSerial {
-    type ReadError = UartError;
-    type WriteError = UartError;
-
-    fn read_byte_with_timeout(
-        &mut self,
-        timeout_ms: u32,
-    ) -> Result<u8, SerialError<Self::ReadError, Self::WriteError>> {
-        let result = G_UART_MAILBOX.wait_until_timeout(timeout_ms);
-        if result {
-            let byte = unsafe { G_UART_RBYTE.pop_front().unwrap() };
-            unsafe { G_UART_MAX_SIZE = max(G_UART_RBYTE.len(), G_UART_MAX_SIZE) };
-            Ok(byte)
-        } else {
-            Err(SerialError::Timeout)
-        }
-    }
-    fn write_byte(
-        &mut self,
-        byte: u8,
-    ) -> Result<(), SerialError<Self::ReadError, Self::WriteError>> {
-        self.tx.write(byte).unwrap();
-        Ok(())
-    }
-}
diff --git a/src/unwind/mod.rs b/src/unwind/mod.rs
index 5a0e008..e5b8d5c 100644
--- a/src/unwind/mod.rs
+++ b/src/unwind/mod.rs
@@ -1,13 +1,13 @@
 #[cfg(feature = "unwind")]
 pub(crate) mod forced;
 #[cfg(feature = "unwind")]
-pub(crate) mod unw_catch;
+pub mod unw_catch;
 #[cfg(feature = "unwind")]
 mod unw_lsda;
 #[cfg(feature = "unwind")]
 mod unw_table;
 #[cfg(feature = "unwind")]
-pub(crate) mod unwind;
+pub mod unwind;
 
 #[cfg(not(feature = "unwind"))]
 mod panic;
diff --git a/src/unwind/unw_lsda.rs b/src/unwind/unw_lsda.rs
index 2e364a6..aeec88e 100644
--- a/src/unwind/unw_lsda.rs
+++ b/src/unwind/unw_lsda.rs
@@ -13,7 +13,6 @@
 #![allow(nonstandard_style)]
 
 use core::{fmt::Formatter, ops::Range};
-use alloc::boxed::Box;
 use fallible_iterator::FallibleIterator;
 use gimli::{constants::*, DwEhPe, EndianSlice, Endianity, Reader};
 
@@ -37,15 +36,7 @@ impl<'input, Endian: Endianity> LSDA<EndianSlice<'input, Endian>> {
     /// The starting address of the function that it corresponds to
     /// must also be provided, because this is often used as the default
     /// base address for the landing pad from which all offsets are calculated.
-    pub fn from_box(data: Box<[u8]>, endian: Endian, function_start_address: u32) -> Self {
-        // let data: Vec<u8> = data.into_vec();
-        let data: &'static [u8] = Box::leak(data);
-        LSDA {
-            reader: EndianSlice::new(data, endian),
-            function_start_address,
-        }
-    }
-    pub fn _new(data: &'input [u8], endian: Endian, function_start_address: u32) -> Self {
+    pub fn new(data: &'input [u8], endian: Endian, function_start_address: u32) -> Self {
         LSDA {
             reader: EndianSlice::new(data, endian),
             function_start_address,
diff --git a/src/unwind/unw_table.rs b/src/unwind/unw_table.rs
index 0d9f3b5..f1ccfd2 100644
--- a/src/unwind/unw_table.rs
+++ b/src/unwind/unw_table.rs
@@ -3,8 +3,6 @@
 //! document: Exception Handling ABI for the Arm Architecture. The chapters
 //! mentioned in below comments all refer to this document.
 
-use alloc::boxed::Box;
-
 use crate::unrecoverable::{self, Lethal};
 
 /// Prel31 offset is a position relative pointer. The value represented
@@ -26,20 +24,13 @@ pub struct Prel31 {
 
 impl Prel31 {
     /// Construct a prel31 offset from little-endian bytes.
-    #[allow(dead_code)]
     pub fn from_bytes(bytes: &[u8; 4]) -> Self {
         Prel31 {
             offset_raw: i32::from_le_bytes(*bytes),
             self_addr: &bytes[0] as *const _ as u32,
         }
     }
-    /// Construct a prel31 offset from little-endian bytes without using the address of the bytes
-    pub fn from_bytes_with_addr(bytes: &[u8; 4], addr: u32) -> Self {
-        Prel31 {
-            offset_raw: i32::from_le_bytes(*bytes),
-            self_addr: addr,
-        }
-    }
+
     /// Get the address it points to.
     pub fn value(&self) -> u32 {
         self.self_addr.wrapping_add(self.offset_value() as u32)
@@ -66,7 +57,7 @@ impl Prel31 {
 /// The type of an exidx entry. There are three variants.
 /// Document chapter 6 (Index table entries).
 #[derive(Debug)]
-enum ExIdxEntryContent {
+enum ExIdxEntryContent<'a> {
     /// The generic variant. The value should be read as a
     /// prel31 offset. We store the decoded address here.
     ExTabEntryAddr(u32),
@@ -76,7 +67,7 @@ enum ExIdxEntryContent {
     /// Bits [23:16], [15:8], [7:0] are the data for the personality
     /// routine. In reality they are ARM unwind instructions.
     /// Other bits are reserved.
-    Compact(PersonalityType, UnwindInstrIter),
+    Compact(PersonalityType, UnwindInstrIter<'a>),
 
     /// The 0x0000_0001 bit pattern, indicating that this
     /// function cannot unwind. No unwind instruction is
@@ -84,7 +75,7 @@ enum ExIdxEntryContent {
     CantUnwind,
 }
 
-impl<'a> ExIdxEntryContent {
+impl<'a> ExIdxEntryContent<'a> {
     /// Extract the exidx entry type from a prel31 offset.
     fn from_raw(prel31: Prel31, bytes: &'a [u8; 4]) -> Self {
         // If the raw pattern is 0x0000_0001, then can't unwind.
@@ -120,20 +111,20 @@ impl<'a> ExIdxEntryContent {
 /// compound content. Entries are sorted according to the function address in
 /// the exidx section.
 #[derive(Debug)]
-pub struct ExIdxEntry {
+pub struct ExIdxEntry<'a> {
     /// The corresponding function address. We store the decoded address
     /// here rather than the raw prel31 offset.
     func_addr: u32,
 
     /// The content enum. It has three variants.
-    content: ExIdxEntryContent,
+    content: ExIdxEntryContent<'a>,
 }
 
-impl<'a> ExIdxEntry {
+impl<'a> ExIdxEntry<'a> {
     /// Construct a new exidx entry from raw bytes. Note that we must read
     /// the .ARM.exidx section in place without any data copy, because
     /// prel31 offset is relative to the address of itself.
-    pub fn _from_bytes(bytes: &'a [u8; 8]) -> Result<Self, &'static str> {
+    pub fn from_bytes(bytes: &'a [u8; 8]) -> Result<Self, &'static str> {
         // Make sure we don't copy anything, just manipulating types.
         let func_offset = Prel31::from_bytes(
             <&[u8; 4]>::try_from(&bytes[0..4])
@@ -157,31 +148,7 @@ impl<'a> ExIdxEntry {
             content: ExIdxEntryContent::from_raw(content_prel31, content_bytes_ref),
         })
     }
-    pub fn from_bytes_with_addr(bytes: &'a [u8; 8], addr: u32) -> Result<Self, &'static str> {
-        // Make sure we don't copy anything, just manipulating types.
-        let func_offset = Prel31::from_bytes_with_addr(
-            <&[u8; 4]>::try_from(&bytes[0..4])
-                .map_err(|_| "ExIdxEntry::from_bytes: array reference conversion failed.")
-                .unwrap_or_die(),
-            addr,
-        );
-        let content_bytes_ref = <&[u8; 4]>::try_from(&bytes[4..8])
-            .map_err(|_| "ExIdxEntry::from_bytes: array reference conversion failed.")
-            .unwrap_or_die();
-        let content_prel31 = Prel31::from_bytes_with_addr(content_bytes_ref, addr + 4);
-
-        // Sanity check. Citing from the document chapter 6:
-        // "The first word contains a prel31 offset to the start
-        // of a function, with bit 31 clear."
-        if func_offset.is_msb_set() {
-            return Err("ExIdxEntry::from_bytes: corrupted entry.");
-        }
 
-        Ok(ExIdxEntry {
-            func_addr: func_offset.value() as u32,
-            content: ExIdxEntryContent::from_raw(content_prel31, content_bytes_ref),
-        })
-    }
     /// Check if the entry describes a function that can unwind.
     pub fn can_unwind(&self) -> bool {
         match self.content {
@@ -202,7 +169,7 @@ impl<'a> ExIdxEntry {
 
     /// Get the unwind instructions from the compact representation.
     /// Precondition: `is_compact()` must return true.
-    pub fn get_unw_instr_iter(&self) -> UnwindInstrIter {
+    pub fn get_unw_instr_iter(&self) -> UnwindInstrIter<'a> {
         match &self.content {
             ExIdxEntryContent::Compact(_, iter) => iter.clone(),
             _ => unrecoverable::die_with_arg("ExIdxEntry::into_unw_instr_iter: not compact."),
@@ -378,32 +345,23 @@ impl UnwindInstruction {
 /// - Inside each word, read from the most significant byte to
 ///   the least significant byte.
 #[derive(Debug, Clone)]
-pub struct UnwindByteIter {
-    bytes: Box<[u8]>,
+pub struct UnwindByteIter<'a> {
+    bytes: &'a [u8],
     pos: usize,
 }
 
-impl<'a> UnwindByteIter {
+impl<'a> UnwindByteIter<'a> {
     /// Create an unwind instruction iterator on its raw byte representation.
     /// Precondition: the byte length must be a multiple of 4.
-    pub fn from_box(bytes: Box<[u8]>) -> Result<Self, &'static str> {
+    fn from_bytes(bytes: &'a [u8]) -> Result<Self, &'static str> {
         if bytes.len() % 4 != 0 {
             return Err("UnwindInstrIter::from_bytes: bytes length not a multiple of 4.");
         }
         Ok(Self { bytes, pos: 0 })
     }
-    pub fn from_bytes(bytes: &'a [u8]) -> Result<Self, &'static str> {
-        if bytes.len() % 4 != 0 {
-            return Err("UnwindInstrIter::from_bytes: bytes length not a multiple of 4.");
-        }
-        Ok(Self {
-            bytes: bytes.to_vec().into_boxed_slice(),
-            pos: 0,
-        })
-    }
 }
 
-impl<'a> Iterator for UnwindByteIter {
+impl<'a> Iterator for UnwindByteIter<'a> {
     type Item = u8;
 
     /// Advance the iterator and return a byte. At 4-byte word level,
@@ -424,11 +382,11 @@ impl<'a> Iterator for UnwindByteIter {
 /// table or the exception indicies. This iterator uses `UnwindByteIter` to get the
 /// raw bytes.
 #[derive(Debug, Clone)]
-pub struct UnwindInstrIter {
-    byte_iter: UnwindByteIter,
+pub struct UnwindInstrIter<'a> {
+    byte_iter: UnwindByteIter<'a>,
 }
 
-impl<'a> Iterator for UnwindInstrIter {
+impl<'a> Iterator for UnwindInstrIter<'a> {
     type Item = UnwindInstruction;
 
     fn next(&mut self) -> Option<Self::Item> {
@@ -439,8 +397,8 @@ impl<'a> Iterator for UnwindInstrIter {
     }
 }
 
-impl<'a> UnwindInstrIter {
-    pub fn from_byte_iter(byte_iter: UnwindByteIter) -> Self {
+impl<'a> UnwindInstrIter<'a> {
+    fn from_byte_iter(byte_iter: UnwindByteIter<'a>) -> Self {
         Self { byte_iter }
     }
 }
@@ -449,17 +407,16 @@ impl<'a> UnwindInstrIter {
 /// followed by a sequence of unwind instructions, and then optionally
 /// a LSDA (language specific data area). This structure only deals with
 /// the language agnostic part, deferring parsing the LSDA to other module.
-#[allow(dead_code)]
-pub struct ExTabEntry {
+pub struct ExTabEntry<'a> {
     /// The personality routine. Can be either the compact model
     /// or the generic model.
     personality: PersonalityType,
 
     /// An iterator that yields unwind instructions.
-    unw_instr_iter: UnwindInstrIter,
+    unw_instr_iter: UnwindInstrIter<'a>,
 }
 
-impl<'a> ExTabEntry {
+impl<'a> ExTabEntry<'a> {
     /// Construct an exception table entry from raw bytes. The `entry_offset` is
     /// the offset into the `.ARM.extab` section. We can obtain it from binary
     /// searching the `.ARM.exidx` section.
@@ -469,7 +426,7 @@ impl<'a> ExTabEntry {
     ///
     /// Reference implementation:
     /// <https://github.com/libunwind/libunwind/blob/e07b43c02d/src/arm/Gex_tables.c>
-    pub fn _from_bytes(
+    pub fn from_bytes(
         extab: &'a [u8],
         entry_offset: usize,
     ) -> Result<(Self, &'a [u8]), &'static str> {
@@ -560,12 +517,12 @@ impl<'a> ExTabEntry {
     }
 
     // Get the personality function.
-    pub fn _get_personality(&self) -> PersonalityType {
+    pub fn get_personality(&self) -> PersonalityType {
         self.personality.clone()
     }
 
     // Get the unwind instruction iterator.
-    pub fn _get_unw_instr_iter(&self) -> UnwindInstrIter {
+    pub fn get_unw_instr_iter(&self) -> UnwindInstrIter<'a> {
         self.unw_instr_iter.clone()
     }
 }
diff --git a/src/unwind/unwind.rs b/src/unwind/unwind.rs
index 11787cf..c9e6cf8 100644
--- a/src/unwind/unwind.rs
+++ b/src/unwind/unwind.rs
@@ -32,27 +32,22 @@
 
 use super::{
     unw_lsda::{self, LSDA},
-    unw_table::{ExIdxEntry, PersonalityType, UnwindByteIter, UnwindInstrIter, UnwindInstruction},
+    unw_table::{
+        ExIdxEntry, ExTabEntry, PersonalityType, Prel31, UnwindInstrIter, UnwindInstruction,
+    },
 };
-#[cfg(any(
-    feature = "unwind_debug",
-    feature = "unwind_print_trace",
-    feature = "offload_unwind"
-))]
-use crate::debug::semihosting::dbg_println;
 use crate::{
     config,
     interrupt::{
-        svc,
+        context_switch, svc,
         svc_handler::SVCNum,
         trap_frame::{self, TrapFrame},
     },
     schedule::{current, scheduler::Scheduler},
     task,
-    uart::{G_TIMEOUT_MS, G_UART_SESSION},
     unrecoverable::{self, Lethal},
 };
-use alloc::{boxed::Box, vec};
+use alloc::boxed::Box;
 use core::{
     alloc::Layout,
     arch::asm,
@@ -67,6 +62,9 @@ use core::{
 };
 use gimli::{EndianSlice, LittleEndian};
 
+#[cfg(any(feature = "unwind_debug", feature = "unwind_print_trace"))]
+use crate::debug::semihosting::dbg_println;
+
 /// If a stack frame can be unwound, its `UnwindInfo` describes how to unwind.
 #[allow(unused)]
 #[derive(Debug)]
@@ -78,7 +76,7 @@ pub struct UnwindInfo<'a> {
     /// personality routine.
     personality: PersonalityType,
     /// The iterator that yields unwind instructions to restore register values.
-    unw_instr_iter: UnwindInstrIter,
+    unw_instr_iter: UnwindInstrIter<'a>,
     /// The LSDA describing the cleanup routines and exception catch blocks.
     /// Compact model that embeds the unwind instructions into the
     /// exidx entry does not have LSDA, thus the field is optional.
@@ -98,12 +96,8 @@ impl<'a> UnwindAbility<'a> {
     /// Arguments:
     /// - `exidx_entry` is the reference to a 2-word entry in the `.ARM.exidx` section.
     /// - `extab` is the slice of the whole `.ARM.extab` section.
-    fn from_bytes(
-        exidx_entry: &[u8; 8],
-        _extab: &'a [u8],
-        entry_addr: u32,
-    ) -> Result<Self, &'static str> {
-        let exidx_entry = ExIdxEntry::from_bytes_with_addr(exidx_entry, entry_addr)?;
+    fn from_bytes(exidx_entry: &'a [u8; 8], extab: &'a [u8]) -> Result<Self, &'static str> {
+        let exidx_entry = ExIdxEntry::from_bytes(exidx_entry)?;
 
         // The current function might not support unwinding.
         if !exidx_entry.can_unwind() {
@@ -124,96 +118,21 @@ impl<'a> UnwindAbility<'a> {
                 lsda: None,
             }))
 
-        // The unwind instructions should be found in the extab followed
+        // The unwind instructions should be find in the extab followed
         // by an LSDA.
         } else {
             let extab_entry_addr = exidx_entry.get_extab_entry_addr() as usize;
-            let session = match unsafe { G_UART_SESSION.as_mut() } {
-                Some(s) => s,
-                None => {
-                    #[cfg(feature = "offload_debug")]
-                    dbg_println!("No session");
-                    return Ok(Self::CantUnwind);
-                }
-            };
-
-            #[cfg(feature = "offload_debug")]
-            dbg_println!("session established, UnwindAbility::from_bytes");
-
-            // Send an extab request to the server.
-            let request_type: u32 = 0xAAAA;
-            let _ = session
-                .send(&request_type.to_le_bytes(), G_TIMEOUT_MS)
-                .unwrap();
-            let _ = session.send(&(extab_entry_addr as u32).to_le_bytes(), G_TIMEOUT_MS).unwrap();
-
-            // in order receive:
-            // 1. The bytes of the ExTabEntry.UnwindInstrIter.UnwindByteIter.bytes (needs to be stepped once)
-            // 2. The lsda_slice as bytes
-            // 3. The personality as u32, with extra byte for generic vs compact
-            let size = session.listen(G_TIMEOUT_MS).unwrap();
-            let mut extab_entry_bytes = vec![0; size as usize].into_boxed_slice();
-
-            let _ = session
-                .receive(&mut extab_entry_bytes, G_TIMEOUT_MS)
-                .unwrap();
-            let mut unw_byte_iter = UnwindByteIter::from_box(extab_entry_bytes).unwrap();
-
-            // need to cycle unw_byte_iter because we lose track of idx between server and MC
-            unw_byte_iter.next();
-
-            let unw_instr_iter = UnwindInstrIter::from_byte_iter(unw_byte_iter);
-
-            #[cfg(feature = "offload_debug")]
-            dbg_println!("Byte iter d : {:?}", unw_instr_iter);
-
-            // 2. The lsda_slice as bytes
-            let size = session.listen(G_TIMEOUT_MS).unwrap();
-            #[cfg(feature = "offload_debug")]
-            dbg_println!("Lsda size : {:?}", size);
-            let mut lsda_slice_bytes = vec![0; size as usize].into_boxed_slice();
-
-            let _ = session
-                .receive(&mut lsda_slice_bytes, G_TIMEOUT_MS)
-                .unwrap();
-
-            let lsda_d = unw_lsda::LSDA::from_box(
-                lsda_slice_bytes,
-                gimli::LittleEndian,
-                exidx_entry.get_func_addr(),
-            );
-            #[cfg(feature = "offload_debug")]
-            dbg_println!("LSDA d : {:?}", lsda_d);
-
-            // 3. The personality as u32, assume always be generic
-            let size = session.listen(G_TIMEOUT_MS).unwrap();
-            assert!(size == 5);
-
-            let mut personality_bytes = [0; 5];
-            let _ = session
-                .receive(&mut personality_bytes, G_TIMEOUT_MS)
-                .unwrap();
-
-            let personality = match personality_bytes[0] {
-                0xAA => {
-                    let p = u32::from_le_bytes(personality_bytes[1..5].try_into().unwrap());
-                    PersonalityType::Compact(p as u8)
-                }
-                0xBB => {
-                    let p = u32::from_le_bytes(personality_bytes[1..5].try_into().unwrap());
-                    PersonalityType::Generic(p)
-                }
-                _ => panic!("Personality type not supported"),
-            };
-
-            #[cfg(feature = "offload_debug")]
-            dbg_println!("Personality d : {:?}\n", personality);
+            let extab_start_addr = &extab[0] as *const u8 as usize;
+            let entry_offset = extab_entry_addr - extab_start_addr;
+            let (extab_entry, lsda_slice) = ExTabEntry::from_bytes(extab, entry_offset)?;
+            let lsda =
+                unw_lsda::LSDA::new(lsda_slice, gimli::LittleEndian, exidx_entry.get_func_addr());
 
             Ok(Self::CanUnwind(UnwindInfo {
                 func_addr: exidx_entry.get_func_addr(),
-                lsda: Some(lsda_d),
-                personality: personality,
-                unw_instr_iter: unw_instr_iter,
+                personality: extab_entry.get_personality(),
+                unw_instr_iter: extab_entry.get_unw_instr_iter(),
+                lsda: Some(lsda),
             }))
         }
     }
@@ -228,50 +147,56 @@ impl<'a> UnwindAbility<'a> {
     pub fn get_for_pc(
         &mut self,
         pc: u32,
-        _exidx: &'a [u8],
-        _extab: &'a [u8],
+        exidx: &'a [u8],
+        extab: &'a [u8],
     ) -> Result<(), &'static str> {
-        let session = match unsafe { G_UART_SESSION.as_mut() } {
-            Some(s) => s,
-            None => {
-                #[cfg(feature = "offload_debug")]
-                dbg_println!("No session");
-                return Ok(());
-            }
-        };
-        #[cfg(feature = "offload_debug")]
-        dbg_println!("pc: {:x?}", pc);
-
-        // send the request type, 0xBBBB for exidx
-        let request_type: u32 = 0xBBBB;
-        let data = request_type.to_le_bytes();
-        let _ = session.send(&data, G_TIMEOUT_MS);
-
-        // send the current pc
-        let data = pc.to_le_bytes();
-        let _ = session.send(&data, G_TIMEOUT_MS).unwrap();
+        if exidx.len() % 8 != 0 {
+            return Err("UnwindAbility::get_for_func: exidx length not multiple of 8.");
+        }
+        if exidx.len() == 0 {
+            return Err("UnwindAbility::get_for_func: empty exidx.");
+        }
 
-        // receive the exidx slice
-        let size = session.listen(G_TIMEOUT_MS).unwrap();
-        let mut exidx_entry = vec![0; size as usize].into_boxed_slice();
-        let _ = session.receive(&mut exidx_entry, G_TIMEOUT_MS).unwrap();
+        // Binary search boundaries.
+        let mut first = 0usize;
+        let mut last = exidx.len() - 8;
 
-        // receive the intended memory address of the exidx slice
-        let size = session.listen(G_TIMEOUT_MS).unwrap();
-        let mut exidx_addr_bytes = vec![0; size as usize].into_boxed_slice();
-        let _ = session
-            .receive(&mut exidx_addr_bytes, G_TIMEOUT_MS)
-            .unwrap();
+        let first_pc =
+            Prel31::from_bytes(<&'a [u8; 4]>::try_from(&exidx[first..first + 4]).unwrap_or_die());
+        if pc < first_pc.value() {
+            return Err("UnwindAbility::get_for_func: no matching entry.");
+        }
 
-        let exidx_addr = u32::from_le_bytes(exidx_addr_bytes[0..4].try_into().unwrap());
+        let last_pc =
+            Prel31::from_bytes(<&'a [u8; 4]>::try_from(&exidx[last..last + 4]).unwrap_or_die());
+        if pc >= last_pc.value() {
+            match Self::from_bytes(
+                <&'a [u8; 8]>::try_from(&exidx[last..last + 8]).unwrap_or_die(),
+                extab,
+            ) {
+                Ok(s) => {
+                    *self = s;
+                    return Ok(());
+                }
+                Err(e) => return Err(e),
+            };
+        }
 
-        #[cfg(feature = "offload_debug")]
-        dbg_println!("exidx_addr: {:x?}", exidx_addr);
+        // Perform binary search.
+        while first < last - 8 {
+            let mid = first + (((last - first) / 8 + 1) >> 1) * 8;
+            let mid_pc =
+                Prel31::from_bytes(<&'a [u8; 4]>::try_from(&exidx[mid..mid + 4]).unwrap_or_die());
+            if pc < mid_pc.value() {
+                last = mid;
+            } else {
+                first = mid;
+            }
+        }
 
         match Self::from_bytes(
-            <&[u8; 8]>::try_from(&exidx_entry[0..8]).unwrap(),
-            _extab,
-            exidx_addr,
+            <&'a [u8; 8]>::try_from(&exidx[first..first + 8]).unwrap_or_die(),
+            extab,
         ) {
             Ok(s) => {
                 *self = s;
@@ -470,7 +395,7 @@ impl<'a> Debug for UnwindState<'a> {
 
 #[inline(never)]
 fn try_concurrent_restart() {
-    current::with_current_task_arc(|cur_task| {
+    current::with_cur_task_arc(|cur_task| {
         // We will limit the concurrent restart rate to at most one concurrent
         // instance. If this task is a restarted instance, and also if the original
         // instance has not finished unwinding, i.e. the task struct reference
@@ -585,7 +510,7 @@ impl UnwindState<'static> {
         // with the current task. There was something wrong with the IRQ
         // handler but do not touch the task.
         if !current::is_in_isr_context() {
-            current::with_current_task(|cur_task| {
+            current::with_cur_task(|cur_task| {
                 if cur_task.is_restartable() {
                     try_concurrent_restart();
                 }
@@ -599,7 +524,7 @@ impl UnwindState<'static> {
             // Let the scheduler re-schedule so the above priority reduction
             // will take effect.
             if !Scheduler::is_suspended() {
-                svc::svc_yield_current_task();
+                context_switch::yield_current_task();
             }
         }
 
@@ -631,10 +556,8 @@ impl UnwindState<'static> {
 
         // Find the unwind ability for the last function before
         // the unwinder is invoked.
-        // let exidx = get_exidx();
-        // let extab = get_extab();
-        let exidx: &[u8; 8] = &[0; 8];
-        let extab: &[u8] = &[0; 0];
+        let exidx = get_exidx();
+        let extab = get_extab();
         unw_state
             .unw_ability
             .get_for_pc(unw_state.gp_regs[ARMGPReg::PC] as u32, exidx, extab)
@@ -796,7 +719,7 @@ impl<'a> UnwindState<'a> {
             self.stklet_boundary = stklet_meta.prev_stklet_bound as u32;
 
             // Update the stack usage.
-            current::with_current_task(|cur_task| {
+            current::with_cur_task(|cur_task| {
                 cur_task.with_stack_ctrl_block(|scb| {
                     scb.cumulated_size
                         .fetch_sub(stklet_meta.count_size, Ordering::SeqCst)
@@ -822,10 +745,8 @@ impl<'a> UnwindState<'a> {
         }
 
         // Update unwind ability information.
-        // let exidx = get_exidx();
-        // let extab = get_extab();
-        let exidx: &[u8; 8] = &[0; 8];
-        let extab: &[u8] = &[0; 0];
+        let exidx = get_exidx();
+        let extab = get_extab();
         self.unw_ability
             .get_for_pc(self.gp_regs[ARMGPReg::PC] as u32, exidx, extab)?;
 
@@ -984,11 +905,11 @@ pub fn is_isr_unwinding() -> bool {
 }
 
 pub fn set_cur_task_unwinding(val: bool) {
-    current::with_current_task(|cur_task| cur_task.set_unwind_flag(val));
+    current::with_cur_task(|cur_task| cur_task.set_unwind_flag(val));
 }
 
 pub fn is_cur_task_unwinding() -> bool {
-    current::with_current_task(|cur_task| cur_task.is_unwinding())
+    current::with_cur_task(|cur_task| cur_task.is_unwinding())
 }
 
 pub fn is_unwinding() -> bool {
@@ -1108,7 +1029,7 @@ unsafe extern "C" fn resume_unwind<'a>(
     if !config::ALLOW_TASK_PREEMPTION {
         if !current::is_in_isr_context() {
             if !Scheduler::is_suspended() {
-                svc::svc_yield_current_task();
+                context_switch::yield_current_task();
             }
         }
     }
@@ -1201,7 +1122,7 @@ unsafe fn panic(_info: &PanicInfo) -> ! {
 }
 
 /// Return the `.ARM.exidx` section as a static byte slice.
-fn _get_exidx() -> &'static [u8] {
+fn get_exidx() -> &'static [u8] {
     extern "C" {
         // These symbols come from `link.ld`
         static __sarm_exidx: u32;
@@ -1227,12 +1148,7 @@ fn _get_exidx() -> &'static [u8] {
 }
 
 /// Return the `.ARM.extab` section as a static byte slice.
-fn _get_extab() -> &'static [u8] {
-    // #[cfg(feature = "offload_unwind")]
-    // {
-    //     dbg_println!("get_extab()");
-    // }
-
+fn get_extab() -> &'static [u8] {
     extern "C" {
         // These symbols come from `link.ld`
         static __sarm_extab: u32;
